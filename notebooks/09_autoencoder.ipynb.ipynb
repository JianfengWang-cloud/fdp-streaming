{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c9ef88-8580-4662-baad-ffaefb4572e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────\n",
      "1. Loading and preparing sequence data...\n",
      "\n",
      "2. Splitting data into Train/History (60%), Validation (20%), and Test (20%) sets...\n",
      "\n",
      "3. Training Autoencoder on 'healthy' data from the Train set...\n",
      "   Training complete.\n",
      "\n",
      "4. Finding optimal anomaly threshold on Validation Set...\n",
      "   Best threshold found: 0.0889 (yields F1=0.0618 on Val Set)\n",
      "\n",
      "5. Final evaluation on unseen Test Set...\n",
      "\n",
      "[Autoencoder Anomaly Detection] Final Test Set Performance:\n",
      "  Used Threshold = 0.0889\n",
      "  F1-Score       = 0.2150\n",
      "  AUC            = 0.7054\n",
      "  G-Mean         = 0.2372\n",
      "  Precision      = 0.1511\n",
      "  Recall         = 0.3725\n",
      "\n",
      "Autoencoder Championship Complete!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# Autoencoder Anomaly Detection Champion\n",
    "#\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"A simple Autoencoder for anomaly detection.\"\"\"\n",
    "    def __init__(self, input_dim, encoding_dim=32):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, encoding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def load_and_prepare_data(config: Dict[str, Any]) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"Loads and prepares the flattened sequence data.\"\"\"\n",
    "    print(\"─\" * 60 + \"\\n1. Loading and preparing sequence data...\")\n",
    "    df = pd.read_csv(config['csv_path']).loc[:, ~pd.read_csv(config['csv_path']).columns.duplicated()]\n",
    "    df[config['quarter_col']] = pd.to_datetime(df[config['quarter_col']])\n",
    "    df.sort_values([config['id_col'], config['quarter_col']], inplace=True)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    feat_cols = [c for c in df.columns if c not in config['meta_cols']]\n",
    "    X, y = [], []\n",
    "    for _, g in df.groupby(config['id_col']):\n",
    "        g = g.sort_values(config['quarter_col'])\n",
    "        arr, lbl = g[feat_cols].to_numpy(), g[config['target_col']].to_numpy()\n",
    "        for i in range(config['lags'], len(g)):\n",
    "            X.append(arr[i - config['lags']:i].ravel())\n",
    "            y.append(lbl[i])\n",
    "            \n",
    "    return np.asarray(X), np.asarray(y)\n",
    "\n",
    "def run_autoencoder_championship(config: Dict[str, Any]):\n",
    "    \"\"\"Orchestrates the entire Autoencoder experiment.\"\"\"\n",
    "    X_all, y_all = load_and_prepare_data(config)\n",
    "    config['n_features'] = X_all.shape[1]\n",
    "    \n",
    "    # --- Step 1: Split data chronologically ---\n",
    "    print(\"\\n2. Splitting data into Train/History (60%), Validation (20%), and Test (20%) sets...\")\n",
    "    n = len(y_all)\n",
    "    train_end = int(n * 0.6)\n",
    "    val_end = int(n * 0.8)\n",
    "    \n",
    "    X_train, y_train = X_all[:train_end], y_all[:train_end]\n",
    "    X_val, y_val = X_all[train_end:val_end], y_all[train_end:val_end]\n",
    "    X_test, y_test = X_all[val_end:], y_all[val_end:]\n",
    "    \n",
    "    # --- Step 2: Train the Autoencoder ONLY on healthy data ---\n",
    "    print(\"\\n3. Training Autoencoder on 'healthy' data from the Train set...\")\n",
    "    X_train_healthy = X_train[y_train == 0]\n",
    "    \n",
    "    # Fit scaler ONLY on this healthy training data\n",
    "    scaler = StandardScaler().fit(X_train_healthy)\n",
    "    X_train_healthy_std = scaler.transform(X_train_healthy)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_train_healthy_std, dtype=torch.float32)), \n",
    "                              batch_size=config['batch_size'], shuffle=True)\n",
    "    \n",
    "    model = Autoencoder(input_dim=config['n_features']).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        for [xb] in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            recon = model(xb)\n",
    "            loss = loss_fn(recon, xb)\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "    print(\"   Training complete.\")\n",
    "    \n",
    "    # --- Step 3: Find the best anomaly threshold on the Validation Set ---\n",
    "    print(\"\\n4. Finding optimal anomaly threshold on Validation Set...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_val_std = scaler.transform(X_val)\n",
    "        val_tensor = torch.tensor(X_val_std, dtype=torch.float32).to(DEVICE)\n",
    "        reconstructions = model(val_tensor)\n",
    "        val_errors = torch.mean((val_tensor - reconstructions)**2, axis=1).cpu().numpy()\n",
    "\n",
    "    best_f1, best_thresh = 0, 0\n",
    "    # Use percentiles of the error distribution as candidate thresholds\n",
    "    for q in np.arange(80, 100, 0.5):\n",
    "        threshold = np.percentile(val_errors, q)\n",
    "        preds = (val_errors >= threshold).astype(int)\n",
    "        current_f1 = f1_score(y_val, preds)\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1, best_thresh = current_f1, threshold\n",
    "    print(f\"   Best threshold found: {best_thresh:.4f} (yields F1={best_f1:.4f} on Val Set)\")\n",
    "\n",
    "    # --- Step 4: Final Exam on the Test Set ---\n",
    "    print(\"\\n5. Final evaluation on unseen Test Set...\")\n",
    "    with torch.no_grad():\n",
    "        X_test_std = scaler.transform(X_test)\n",
    "        test_tensor = torch.tensor(X_test_std, dtype=torch.float32).to(DEVICE)\n",
    "        reconstructions = model(test_tensor)\n",
    "        test_errors = torch.mean((test_tensor - reconstructions)**2, axis=1).cpu().numpy()\n",
    "    \n",
    "    final_preds = (test_errors >= best_thresh).astype(int)\n",
    "    \n",
    "    final_f1 = f1_score(y_test, final_preds, zero_division=0)\n",
    "    final_prec = precision_score(y_test, final_preds, zero_division=0)\n",
    "    final_rec = recall_score(y_test, final_preds, zero_division=0)\n",
    "    final_auc = roc_auc_score(y_test, test_errors)\n",
    "    final_gmean = np.sqrt(final_prec * final_rec) if final_prec > 0 and final_rec > 0 else 0\n",
    "\n",
    "    print(f\"\\n[Autoencoder Anomaly Detection] Final Test Set Performance:\")\n",
    "    print(f\"  Used Threshold = {best_thresh:.4f}\")\n",
    "    print(f\"  F1-Score       = {final_f1:.4f}\")\n",
    "    print(f\"  AUC            = {final_auc:.4f}\")\n",
    "    print(f\"  G-Mean         = {final_gmean:.4f}\")\n",
    "    print(f\"  Precision      = {final_prec:.4f}\")\n",
    "    print(f\"  Recall         = {final_rec:.4f}\")\n",
    "    print(\"\\nAutoencoder Championship Complete!\")\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {\n",
    "        \"csv_path\": r'cvm_indicators_dataset_2011-2021.csv',\n",
    "        \"id_col\": \"ID\", \"quarter_col\": \"QUARTER\", \"target_col\": \"LABEL\",\n",
    "        \"meta_cols\": [\"ID\", \"QUARTER\", \"LABEL\"],\n",
    "        \"lags\": 4,\n",
    "        \n",
    "        # Autoencoder specific parameters\n",
    "        \"epochs\": 75, # Needs more epochs to learn the distribution well\n",
    "        \"batch_size\": 256,\n",
    "        \"lr\": 0.001,\n",
    "    }\n",
    "    \n",
    "    run_autoencoder_championship(config=CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14905bd1-3121-4d22-becb-7d0fec4a2f82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
