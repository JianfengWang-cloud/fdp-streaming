{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7059f1-0180-4155-9860-51f9705fbef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Using cached imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in d:\\anaconda\\envs\\gpu_env\\lib\\site-packages (from imbalanced-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in d:\\anaconda\\envs\\gpu_env\\lib\\site-packages (from imbalanced-learn) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in d:\\anaconda\\envs\\gpu_env\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n",
      "  Using cached sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in d:\\anaconda\\envs\\gpu_env\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in d:\\anaconda\\envs\\gpu_env\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Using cached imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "Using cached sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn\n",
      "\n",
      "   -------------------- ------------------- 1/2 [imbalanced-learn]\n",
      "   -------------------- ------------------- 1/2 [imbalanced-learn]\n",
      "   -------------------- ------------------- 1/2 [imbalanced-learn]\n",
      "   -------------------- ------------------- 1/2 [imbalanced-learn]\n",
      "   ---------------------------------------- 2/2 [imbalanced-learn]\n",
      "\n",
      "Successfully installed imbalanced-learn-0.13.0 sklearn-compat-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcdd3a02-56ed-4fa0-882f-89871f4ca6b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────\n",
      "1. Loading and cleaning data...\n",
      "2. Preparing sequence data...\n",
      "3. Splitting data into Tune (60%), Validation (20%), and Test (20%) sets...\n",
      "   Tune set size: 12256\n",
      "   Validation set size: 4086\n",
      "   Test set size: 4086\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "Bake-Off Round 1: Evaluating Expert-Tuned Baseline Model with SMOTE\n",
      "════════════════════════════════════════════════════════════\n",
      "\n",
      "--- Evaluating 'Expert-Tuned LGBM + SMOTE' on the Final Test Set ---\n",
      "  Retraining at test step 0...\n",
      "  Retraining at test step 500...\n",
      "  Retraining at test step 1000...\n",
      "    Skipping training: not enough positive samples for SMOTE.\n",
      "  Retraining at test step 1500...\n",
      "    Skipping training: not enough positive samples for SMOTE.\n",
      "  Retraining at test step 2000...\n",
      "  Retraining at test step 2500...\n",
      "  Retraining at test step 3000...\n",
      "  Retraining at test step 3500...\n",
      "    Skipping training: not enough positive samples for SMOTE.\n",
      "  Retraining at test step 4000...\n",
      "    Skipping training: not enough positive samples for SMOTE.\n",
      "\n",
      "  Tuning classification threshold and calculating all metrics...\n",
      "\n",
      "[Expert-Tuned LGBM + SMOTE] Final Test Set Performance:\n",
      "  Best Threshold = 0.68\n",
      "  F1-Score       = 0.2443\n",
      "  AUC            = 0.6775\n",
      "  G-Mean         = 0.2520\n",
      "  Precision      = 0.1961\n",
      "  Recall         = 0.3239\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "Bake-Off Round 2: Finding and Evaluating Optuna-Tuned Model with SMOTE\n",
      "════════════════════════════════════════════════════════════\n",
      "4. Starting Optuna optimization process...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88136965cb7403bbafe7d054e7e00a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna process finished!\n",
      "🏆 Best F1-score on Validation Set: 0.5351\n",
      "🏆 Best Hyperparameters Found: {'n_estimators': 400, 'learning_rate': 0.019551686884351806, 'num_leaves': 150, 'reg_alpha': 8.032889994875912, 'reg_lambda': 8.219147923973554e-05}\n",
      "\n",
      "--- Evaluating 'Optuna-Tuned LGBM + SMOTE' on the Final Test Set ---\n",
      "  Retraining at test step 0...\n",
      "  Retraining at test step 500...\n",
      "  Retraining at test step 1000...\n",
      "    Skipping training: not enough positive samples for SMOTE.\n",
      "  Retraining at test step 1500...\n",
      "    Skipping training: not enough positive samples for SMOTE.\n",
      "  Retraining at test step 2000...\n",
      "  Retraining at test step 2500...\n",
      "  Retraining at test step 3000...\n",
      "  Retraining at test step 3500...\n",
      "    Skipping training: not enough positive samples for SMOTE.\n",
      "  Retraining at test step 4000...\n",
      "    Skipping training: not enough positive samples for SMOTE.\n",
      "\n",
      "  Tuning classification threshold and calculating all metrics...\n",
      "\n",
      "[Optuna-Tuned LGBM + SMOTE] Final Test Set Performance:\n",
      "  Best Threshold = 0.36\n",
      "  F1-Score       = 0.2113\n",
      "  AUC            = 0.6577\n",
      "  G-Mean         = 0.2909\n",
      "  Precision      = 0.1252\n",
      "  Recall         = 0.6761\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "Bake-Off Complete!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# LightGBM + SMOTE Champion Model Bake-Off v2 (Robust)\n",
    "#\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "class LGBM_SMOTE_Champion_Finder:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.df = self._load_data(config['csv_path'])\n",
    "        self.feat_cols = [c for c in self.df.columns if c not in config['meta_cols']]\n",
    "        self.X_all, self.y_all = self._make_windows()\n",
    "        \n",
    "        self.X_tune, self.y_tune, \\\n",
    "        self.X_val, self.y_val, \\\n",
    "        self.X_test, self.y_test = self._split_data()\n",
    "\n",
    "    def _load_data(self, path: str | Path) -> pd.DataFrame:\n",
    "        print(\"─\" * 60 + \"\\n1. Loading and cleaning data...\")\n",
    "        df = pd.read_csv(path).loc[:, ~pd.read_csv(path).columns.duplicated()]\n",
    "        req = set(self.config['meta_cols'])\n",
    "        if missing := req - set(df.columns): raise KeyError(f\"Missing cols: {missing}\")\n",
    "        df[self.config['quarter_col']] = pd.to_datetime(df[self.config['quarter_col']])\n",
    "        df.sort_values([self.config['id_col'], self.config['quarter_col']], inplace=True)\n",
    "        df = df.dropna()\n",
    "        num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        return df[list(req | set(num_cols))]\n",
    "\n",
    "    def _make_windows(self) -> (np.ndarray, np.ndarray):\n",
    "        print(\"2. Preparing sequence data...\")\n",
    "        X, y = [], []\n",
    "        cfg = self.config\n",
    "        for _, g in self.df.groupby(cfg['id_col']):\n",
    "            g = g.sort_values(cfg['quarter_col'])\n",
    "            arr, lbl = g[self.feat_cols].to_numpy(), g[cfg['target_col']].to_numpy()\n",
    "            for i in range(cfg['lags'], len(g)):\n",
    "                X.append(arr[i - cfg['lags']:i].ravel())\n",
    "                y.append(lbl[i])\n",
    "        return np.asarray(X), np.asarray(y)\n",
    "\n",
    "    def _split_data(self):\n",
    "        print(\"3. Splitting data into Tune (60%), Validation (20%), and Test (20%) sets...\")\n",
    "        n = len(self.y_all)\n",
    "        tune_end = int(n * 0.6)\n",
    "        val_end = int(n * 0.8)\n",
    "        \n",
    "        X_tune, y_tune = self.X_all[:tune_end], self.y_all[:tune_end]\n",
    "        X_val, y_val = self.X_all[tune_end:val_end], self.y_all[tune_end:val_end]\n",
    "        X_test, y_test = self.X_all[val_end:], self.y_all[val_end:]\n",
    "        \n",
    "        print(f\"   Tune set size: {len(y_tune)}\")\n",
    "        print(f\"   Validation set size: {len(y_val)}\")\n",
    "        print(f\"   Test set size: {len(y_test)}\")\n",
    "        return X_tune, y_tune, X_val, y_val, X_test, y_test\n",
    "\n",
    "    def _objective(self, trial: optuna.Trial) -> float:\n",
    "        params = {\n",
    "            'objective': 'binary', 'metric': 'binary_logloss', 'verbosity': -1,\n",
    "            'random_state': self.config['seed'],\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 200, 800, step=100),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-2, 0.1, log=True),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        }\n",
    "\n",
    "        # 🔥 FIX: Adaptive k_neighbors for SMOTE in Optuna trials\n",
    "        n_pos_in_tune = np.sum(self.y_tune)\n",
    "        k_neighbors = min(5, n_pos_in_tune - 1)\n",
    "        \n",
    "        if k_neighbors < 1: # SMOTE cannot run\n",
    "            return 0.0 # Return a bad score if SMOTE is impossible\n",
    "            \n",
    "        smote = SMOTE(random_state=self.config['seed'], k_neighbors=k_neighbors)\n",
    "        X_tune_resampled, y_tune_resampled = smote.fit_resample(self.X_tune, self.y_tune)\n",
    "        \n",
    "        scaler = StandardScaler().fit(self.X_tune)\n",
    "        X_tune_std = scaler.transform(X_tune_resampled)\n",
    "        X_val_std = scaler.transform(self.X_val)\n",
    "\n",
    "        model = lgb.LGBMClassifier(**params).fit(X_tune_std, y_tune_resampled)\n",
    "        \n",
    "        y_probs = model.predict_proba(X_val_std)[:, 1]\n",
    "        \n",
    "        best_f1 = 0\n",
    "        for threshold in np.arange(0.1, 0.9, 0.05):\n",
    "            preds = (y_probs > threshold).astype(int)\n",
    "            best_f1 = max(best_f1, f1_score(self.y_val, preds))\n",
    "            \n",
    "        return best_f1\n",
    "    \n",
    "    def _evaluate_on_test_set(self, params: Dict[str, Any], model_name: str):\n",
    "        print(f\"\\n--- Evaluating '{model_name}' on the Final Test Set ---\")\n",
    "        \n",
    "        win_size, retrain_interval = self.config['sliding_win_size'], self.config['retrain_interval']\n",
    "        X_history = np.vstack([self.X_tune, self.X_val])\n",
    "        y_history = np.concatenate([self.y_tune, self.y_val])\n",
    "        \n",
    "        all_probs, all_trues = [], []\n",
    "        model, scaler = None, None\n",
    "\n",
    "        for i in range(len(self.X_test)):\n",
    "            if model is None or i % retrain_interval == 0:\n",
    "                print(f\"  Retraining at test step {i}...\")\n",
    "                \n",
    "                X_train_current = np.vstack([X_history, self.X_test[:i]])\n",
    "                y_train_current = np.concatenate([y_history, self.y_test[:i]])\n",
    "                \n",
    "                X_train_window, y_train_window = X_train_current[-win_size:], y_train_current[-win_size:]\n",
    "\n",
    "                n_pos_in_window = np.sum(y_train_window)\n",
    "                if n_pos_in_window < 2:\n",
    "                    print(\"    Skipping training: not enough positive samples for SMOTE.\")\n",
    "                    continue\n",
    "\n",
    "                # 🔥 FIX: Adaptive k_neighbors for SMOTE in the sliding window\n",
    "                k_neighbors = min(5, n_pos_in_window - 1)\n",
    "                smote = SMOTE(random_state=self.config['seed'], k_neighbors=k_neighbors)\n",
    "                X_resampled, y_resampled = smote.fit_resample(X_train_window, y_train_window)\n",
    "\n",
    "                scaler = StandardScaler().fit(X_train_window)\n",
    "                X_train_std = scaler.transform(X_resampled)\n",
    "                model = lgb.LGBMClassifier(**params).fit(X_train_std, y_resampled)\n",
    "\n",
    "            if model is None: continue\n",
    "\n",
    "            X_test_point = self.X_test[i].reshape(1, -1)\n",
    "            X_test_point_std = scaler.transform(X_test_point)\n",
    "            \n",
    "            y_prob = model.predict_proba(X_test_point_std)[:, 1][0]\n",
    "            all_probs.append(y_prob)\n",
    "            all_trues.append(self.y_test[i])\n",
    "            \n",
    "        print(\"\\n  Tuning classification threshold and calculating all metrics...\")\n",
    "        \n",
    "        final_auc = roc_auc_score(all_trues, all_probs)\n",
    "        best_f1, best_thresh, best_prec, best_rec = 0, 0, 0, 0\n",
    "        \n",
    "        for threshold in np.arange(0.1, 0.9, 0.01):\n",
    "            preds = (np.array(all_probs) > threshold).astype(int)\n",
    "            current_f1 = f1_score(all_trues, preds, zero_division=0)\n",
    "            if current_f1 > best_f1:\n",
    "                best_f1, best_thresh = current_f1, threshold\n",
    "                best_prec = precision_score(all_trues, preds, zero_division=0)\n",
    "                best_rec = recall_score(all_trues, preds, zero_division=0)\n",
    "        \n",
    "        final_gmean = np.sqrt(best_prec * best_rec) if best_prec > 0 and best_rec > 0 else 0\n",
    "\n",
    "        print(f\"\\n[{model_name}] Final Test Set Performance:\")\n",
    "        print(f\"  Best Threshold = {best_thresh:.2f}\")\n",
    "        print(f\"  F1-Score       = {best_f1:.4f}\")\n",
    "        print(f\"  AUC            = {final_auc:.4f}\")\n",
    "        print(f\"  G-Mean         = {final_gmean:.4f}\")\n",
    "        print(f\"  Precision      = {best_prec:.4f}\")\n",
    "        print(f\"  Recall         = {best_rec:.4f}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Orchestrates the entire bake-off process.\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"═\" * 60)\n",
    "        print(\"Bake-Off Round 1: Evaluating Expert-Tuned Baseline Model with SMOTE\")\n",
    "        print(\"═\" * 60)\n",
    "        expert_params = self.config['lightgbm_expert_params']\n",
    "        self._evaluate_on_test_set(expert_params, \"Expert-Tuned LGBM + SMOTE\")\n",
    "\n",
    "        print(\"\\n\" + \"═\" * 60)\n",
    "        print(\"Bake-Off Round 2: Finding and Evaluating Optuna-Tuned Model with SMOTE\")\n",
    "        print(\"═\" * 60)\n",
    "        print(\"4. Starting Optuna optimization process...\")\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(self._objective, n_trials=self.config['optuna_trials'], show_progress_bar=True)\n",
    "        \n",
    "        print(f\"\\nOptuna process finished!\")\n",
    "        print(f\"🏆 Best F1-score on Validation Set: {study.best_value:.4f}\")\n",
    "        print(f\"🏆 Best Hyperparameters Found: {study.best_params}\")\n",
    "        \n",
    "        optuna_params = {**self.config['lightgbm_expert_params'], **study.best_params}\n",
    "        self._evaluate_on_test_set(optuna_params, \"Optuna-Tuned LGBM + SMOTE\")\n",
    "        print(\"\\n\" + \"═\" * 60)\n",
    "        print(\"Bake-Off Complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {\n",
    "        \"csv_path\": r'cvm_indicators_dataset_2011-2021.csv',\n",
    "        \"id_col\": \"ID\", \"quarter_col\": \"QUARTER\", \"target_col\": \"LABEL\",\n",
    "        \"meta_cols\": [\"ID\", \"QUARTER\", \"LABEL\"],\n",
    "        \"lags\": 4, \"seed\": 42,\n",
    "        \n",
    "        \"sliding_win_size\": 200,\n",
    "        \"retrain_interval\": 500,\n",
    "        \"optuna_trials\": 50,\n",
    "\n",
    "        \"lightgbm_expert_params\": {\n",
    "            \"objective\": \"binary\", \"metric\": \"auc\", \"random_state\": 42, \n",
    "            \"n_estimators\": 500, \"learning_rate\": 0.05,\n",
    "            \"verbose\": -1\n",
    "        },\n",
    "    }\n",
    "\n",
    "    champion_finder = LGBM_SMOTE_Champion_Finder(config=CONFIG)\n",
    "    champion_finder.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a8aae2-e663-47a8-92f4-a3dfae7d42a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
