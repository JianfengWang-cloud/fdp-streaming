{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b20bccb-ff3f-4fef-864d-046590c1fdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Loading and cleaning data...\n",
      "2. Preparing sequence data...\n",
      "3. Splitting data into Tune (60%), Validation (20%), and Test (20%) sets...\n",
      "   Tune set size: 12256\n",
      "   Validation set size: 4086\n",
      "   Test set size: 4086\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "Starting LSTM with Attention Championship Bake-Off\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "4. Starting Optuna optimization process...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7798be9dd0b445338f98bb5c23151598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna process finished!\n",
      "ğŸ† Best F1-score on Validation Set: 0.4423\n",
      "ğŸ† Best Hyperparameters Found: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.4011674911272012, 'lr': 0.0012991481463840325}\n",
      "\n",
      "--- Training and Evaluating Champion LSTM Model ---\n",
      "Final training on 16342 samples...\n",
      "Final training complete.\n",
      "\n",
      "  Tuning classification threshold and calculating all metrics on test set...\n",
      "\n",
      "[Optuna-Tuned LSTM] Final Test Set Performance:\n",
      "  Best Threshold = 0.15\n",
      "  F1-Score       = 0.3388\n",
      "  AUC            = 0.7910\n",
      "  G-Mean         = 0.3454\n",
      "  Precision      = 0.2834\n",
      "  Recall         = 0.4211\n",
      "\n",
      "LSTM Bake-Off Complete!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# LSTM with Attention Champion Model Bake-Off with Optuna\n",
    "#\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Suppress Optuna's trial info messages and other warnings\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LSTMWithAttention(nn.Module):\n",
    "    \"\"\"An LSTM with a basic attention mechanism.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers,\n",
    "            batch_first=True, dropout=dropout, bidirectional=False\n",
    "        )\n",
    "        self.attn_layer = nn.Linear(hidden_size, 1)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn_weights = torch.softmax(self.attn_layer(lstm_out), dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        # We apply sigmoid here because we'll use BCELoss\n",
    "        output = self.sigmoid(self.output_layer(context))\n",
    "        return output\n",
    "\n",
    "class LSTM_Champion_Finder:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.df = self._load_data(config['csv_path'])\n",
    "        self.feat_cols = [c for c in self.df.columns if c not in config['meta_cols']]\n",
    "        self.X_all, self.y_all = self._make_windows()\n",
    "        \n",
    "        self.X_tune, self.y_tune, \\\n",
    "        self.X_val, self.y_val, \\\n",
    "        self.X_test, self.y_test = self._split_data()\n",
    "        \n",
    "        self.scaler = StandardScaler().fit(self.X_tune.reshape(-1, self.config['n_features']))\n",
    "        self.X_tune_std = self._scale_data(self.X_tune)\n",
    "        self.X_val_std = self._scale_data(self.X_val)\n",
    "        self.X_test_std = self._scale_data(self.X_test)\n",
    "\n",
    "    def _load_data(self, path: str | Path) -> pd.DataFrame:\n",
    "        print(\"â”€\" * 60 + \"\\n1. Loading and cleaning data...\")\n",
    "        df = pd.read_csv(path).loc[:, ~pd.read_csv(path).columns.duplicated()]\n",
    "        req = set(self.config['meta_cols'])\n",
    "        if missing := req - set(df.columns): raise KeyError(f\"Missing cols: {missing}\")\n",
    "        df[self.config['quarter_col']] = pd.to_datetime(df[self.config['quarter_col']])\n",
    "        df.sort_values([self.config['id_col'], self.config['quarter_col']], inplace=True)\n",
    "        df = df.dropna()\n",
    "        num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        return df[list(req | set(num_cols))]\n",
    "    \n",
    "    def _make_windows(self) -> (np.ndarray, np.ndarray):\n",
    "        print(\"2. Preparing sequence data...\")\n",
    "        X, y = [], []\n",
    "        cfg = self.config\n",
    "        for _, g in self.df.groupby(cfg['id_col']):\n",
    "            g = g.sort_values(cfg['quarter_col'])\n",
    "            arr, lbl = g[self.feat_cols].to_numpy(), g[cfg['target_col']].to_numpy()\n",
    "            for i in range(cfg['lags'], len(g)):\n",
    "                X.append(arr[i - cfg['lags']:i])\n",
    "                y.append(lbl[i])\n",
    "        return np.asarray(X), np.asarray(y)\n",
    "\n",
    "    def _split_data(self):\n",
    "        print(\"3. Splitting data into Tune (60%), Validation (20%), and Test (20%) sets...\")\n",
    "        n = len(self.y_all)\n",
    "        tune_end = int(n * 0.6)\n",
    "        val_end = int(n * 0.8)\n",
    "        \n",
    "        X_tune, y_tune = self.X_all[:tune_end], self.y_all[:tune_end]\n",
    "        X_val, y_val = self.X_all[tune_end:val_end], self.y_all[tune_end:val_end]\n",
    "        X_test, y_test = self.X_all[val_end:], self.y_all[val_end:]\n",
    "        \n",
    "        print(f\"   Tune set size: {len(y_tune)}\")\n",
    "        print(f\"   Validation set size: {len(y_val)}\")\n",
    "        print(f\"   Test set size: {len(y_test)}\")\n",
    "        return X_tune, y_tune, X_val, y_val, X_test, y_test\n",
    "\n",
    "    def _scale_data(self, X):\n",
    "        return self.scaler.transform(X.reshape(-1, self.config['n_features'])).reshape(X.shape)\n",
    "\n",
    "    def _objective(self, trial: optuna.Trial) -> float:\n",
    "        \"\"\"The objective function for Optuna to maximize.\"\"\"\n",
    "        cfg = self.config\n",
    "        params = {\n",
    "            'hidden_size': trial.suggest_categorical('hidden_size', [32, 64, 128]),\n",
    "            'num_layers': trial.suggest_int('num_layers', 1, 3),\n",
    "            'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "            'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "        }\n",
    "        \n",
    "        model = LSTMWithAttention(\n",
    "            input_size=cfg['n_features'], \n",
    "            hidden_size=params['hidden_size'],\n",
    "            num_layers=params['num_layers'],\n",
    "            dropout=params['dropout']\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(torch.tensor(self.X_tune_std, dtype=torch.float32), \n",
    "                                                torch.tensor(self.y_tune, dtype=torch.float32).unsqueeze(1)),\n",
    "                                  batch_size=cfg['batch_size'], shuffle=True)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        # Using standard BCELoss because model's forward pass includes Sigmoid\n",
    "        loss_fn = nn.BCELoss()\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(cfg['epochs']):\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                preds = model(xb)\n",
    "                loss = loss_fn(preds, yb)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_tensor = torch.tensor(self.X_val_std, dtype=torch.float32).to(DEVICE)\n",
    "            val_probs = model(val_tensor).cpu().numpy().flatten()\n",
    "            \n",
    "        best_f1 = 0\n",
    "        for threshold in np.arange(0.1, 0.9, 0.05):\n",
    "            preds = (val_probs > threshold).astype(int)\n",
    "            best_f1 = max(best_f1, f1_score(self.y_val, preds))\n",
    "        \n",
    "        return best_f1\n",
    "\n",
    "    def _evaluate_champion_model(self, params: Dict[str, Any]):\n",
    "        \"\"\"Trains the champion LSTM on all history and evaluates on the test set.\"\"\"\n",
    "        print(\"\\n--- Training and Evaluating Champion LSTM Model ---\")\n",
    "        cfg = self.config\n",
    "        \n",
    "        X_train_final = np.vstack([self.X_tune_std, self.X_val_std])\n",
    "        y_train_final = np.concatenate([self.y_tune, self.y_val])\n",
    "\n",
    "        print(f\"Final training on {len(y_train_final)} samples...\")\n",
    "        \n",
    "        final_loader = DataLoader(TensorDataset(torch.tensor(X_train_final, dtype=torch.float32), \n",
    "                                                torch.tensor(y_train_final, dtype=torch.float32).unsqueeze(1)),\n",
    "                                  batch_size=cfg['batch_size'], shuffle=True)\n",
    "\n",
    "        model = LSTMWithAttention(\n",
    "            input_size=cfg['n_features'],\n",
    "            hidden_size=params['hidden_size'],\n",
    "            num_layers=params['num_layers'],\n",
    "            dropout=params['dropout']\n",
    "        ).to(DEVICE)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        loss_fn = nn.BCELoss()\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(cfg['epochs_final']):\n",
    "            for xb, yb in final_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                loss = loss_fn(model(xb), yb)\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        print(\"Final training complete.\")\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_tensor = torch.tensor(self.X_test_std, dtype=torch.float32).to(DEVICE)\n",
    "            test_probs = model(test_tensor).cpu().numpy().flatten()\n",
    "        \n",
    "        print(\"\\n  Tuning classification threshold and calculating all metrics on test set...\")\n",
    "        final_auc = roc_auc_score(self.y_test, test_probs)\n",
    "        best_f1, best_thresh, best_prec, best_rec = 0, 0, 0, 0\n",
    "        for threshold in np.arange(0.1, 0.9, 0.01):\n",
    "            preds = (test_probs > threshold).astype(int)\n",
    "            current_f1 = f1_score(self.y_test, preds, zero_division=0)\n",
    "            if current_f1 > best_f1:\n",
    "                best_f1, best_thresh = current_f1, threshold\n",
    "                best_prec = precision_score(self.y_test, preds, zero_division=0)\n",
    "                best_rec = recall_score(self.y_test, preds, zero_division=0)\n",
    "        final_gmean = np.sqrt(best_prec * best_rec) if best_prec > 0 and best_rec > 0 else 0\n",
    "\n",
    "        print(f\"\\n[Optuna-Tuned LSTM] Final Test Set Performance:\")\n",
    "        print(f\"  Best Threshold = {best_thresh:.2f}\")\n",
    "        print(f\"  F1-Score       = {best_f1:.4f}\")\n",
    "        print(f\"  AUC            = {final_auc:.4f}\")\n",
    "        print(f\"  G-Mean         = {final_gmean:.4f}\")\n",
    "        print(f\"  Precision      = {best_prec:.4f}\")\n",
    "        print(f\"  Recall         = {best_rec:.4f}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Orchestrates the entire LSTM bake-off process.\"\"\"\n",
    "        print(\"\\n\" + \"â•\" * 60)\n",
    "        print(\"Starting LSTM with Attention Championship Bake-Off\")\n",
    "        print(\"â•\" * 60)\n",
    "        \n",
    "        print(\"4. Starting Optuna optimization process...\")\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(self._objective, n_trials=self.config['optuna_trials'], show_progress_bar=True)\n",
    "        \n",
    "        print(f\"\\nOptuna process finished!\")\n",
    "        print(f\"ğŸ† Best F1-score on Validation Set: {study.best_value:.4f}\")\n",
    "        print(f\"ğŸ† Best Hyperparameters Found: {study.best_params}\")\n",
    "        \n",
    "        self._evaluate_champion_model(study.best_params)\n",
    "        print(\"\\nLSTM Bake-Off Complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    temp_df = pd.read_csv(r'cvm_indicators_dataset_2011-2021.csv')\n",
    "    n_features = len(temp_df.columns) - 3 \n",
    "\n",
    "    CONFIG = {\n",
    "        \"csv_path\": r'cvm_indicators_dataset_2011-2021.csv',\n",
    "        \"id_col\": \"ID\", \"quarter_col\": \"QUARTER\", \"target_col\": \"LABEL\",\n",
    "        \"meta_cols\": [\"ID\", \"QUARTER\", \"LABEL\"],\n",
    "        \"lags\": 4, \"seed\": 42,\n",
    "        \"n_features\": n_features,\n",
    "        \n",
    "        \"optuna_trials\": 30,\n",
    "\n",
    "        \"epochs\": 15,\n",
    "        \"epochs_final\": 25,\n",
    "        \"batch_size\": 128,\n",
    "    }\n",
    "\n",
    "    champion_finder = LSTM_Champion_Finder(config=CONFIG)\n",
    "    champion_finder.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2533eb14-4933-44ed-8e8a-b556ceca0f29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
