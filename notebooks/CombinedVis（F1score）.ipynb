{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b81c0-b9a7-4186-bfe0-b536fe6cc9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading and cleaning data...\n",
      "2. Preparing flattened data windows...\n",
      "2. Preparing sequential data windows...\n",
      "✅ Data prepared. Total sequences: 20428\n",
      "✅ Using device: cuda\n",
      "\n",
      "--- Running Naive Bayes (per-instance retraining) ---\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# Final Analysis: Visualizing Concept Drift - v2 (Corrected Plotting)\n",
    "#\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from river import ensemble, tree, drift\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.attn_layer = nn.Linear(hidden_size, 1)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn_weights = torch.softmax(self.attn_layer(lstm_out), dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        return self.sigmoid(self.output_layer(context))\n",
    "\n",
    "class Final_Drift_Visualizer:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.df = self._load_data(config['csv_path'])\n",
    "        self.feat_cols = [c for c in self.df.columns if c not in config['meta_cols']]\n",
    "        \n",
    "        self.X_flat, self.y_flat = self._make_windows(flatten=True)\n",
    "        self.X_seq, self.y_seq = self._make_windows(flatten=False)\n",
    "        self.config['n_features_seq'] = self.X_seq.shape[2]\n",
    "        \n",
    "        print(f\"✅ Data prepared. Total sequences: {len(self.y_flat)}\")\n",
    "        print(f\"✅ Using device: {DEVICE}\")\n",
    "\n",
    "    def _load_data(self, path: str | Path) -> pd.DataFrame:\n",
    "        print(\"1. Loading and cleaning data...\")\n",
    "        df = pd.read_csv(path).loc[:, ~pd.read_csv(path).columns.duplicated()]\n",
    "        req = set(self.config['meta_cols'])\n",
    "        if missing := req - set(df.columns): raise KeyError(f\"Missing cols: {missing}\")\n",
    "        df[self.config['quarter_col']] = pd.to_datetime(df[self.config['quarter_col']])\n",
    "        df.sort_values([self.config['id_col'], self.config['quarter_col']], inplace=True)\n",
    "        return df.dropna()\n",
    "\n",
    "    def _make_windows(self, flatten: bool) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        print(f\"2. Preparing {'flattened' if flatten else 'sequential'} data windows...\")\n",
    "        X, y = [], []\n",
    "        cfg = self.config\n",
    "        for _, g in self.df.groupby(cfg['id_col']):\n",
    "            arr, lbl = g[self.feat_cols].to_numpy(), g[cfg['target_col']].to_numpy()\n",
    "            for i in range(cfg['lags'], len(g)):\n",
    "                win = arr[i - cfg['lags']:i]\n",
    "                X.append(win.ravel() if flatten else win)\n",
    "                y.append(lbl[i])\n",
    "        return np.asarray(X), np.asarray(y)\n",
    "    \n",
    "    def _get_rolling_f1(self, y_trues: List[int], y_preds: List[int], step: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        f1_scores, indices = [], []\n",
    "        for i in range(step, len(y_trues), step):\n",
    "            window_trues = y_trues[:i]\n",
    "            window_preds = y_preds[:i]\n",
    "            f1 = f1_score(window_trues, window_preds, zero_division=0)\n",
    "            f1_scores.append(f1)\n",
    "            indices.append(i)\n",
    "        return np.array(indices), np.array(f1_scores)\n",
    "\n",
    "    def run_naive_bayes(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        print(\"\\n--- Running Naive Bayes (per-instance retraining) ---\")\n",
    "        X, y = self.X_flat, self.y_flat\n",
    "        cfg = self.config\n",
    "        win_size, history_end = cfg['nb_win_size'], int(len(y) * 0.2)\n",
    "        X_history, y_history, X_test, y_test = X[:history_end], y[:history_end], X[history_end:], y[history_end:]\n",
    "        preds, trues = [], []\n",
    "        for i in range(len(X_test)):\n",
    "            current_train_X = np.vstack([X_history, X_test[:i]])[-win_size:]\n",
    "            current_train_y = np.concatenate([y_history, y_test[:i]])[-win_size:]\n",
    "            if len(np.unique(current_train_y)) < 2:\n",
    "                preds.append(0); trues.append(y_test[i]); continue\n",
    "            scaler = StandardScaler().fit(current_train_X)\n",
    "            model = GaussianNB().fit(scaler.transform(current_train_X), current_train_y)\n",
    "            y_prob = model.predict_proba(scaler.transform(X_test[i].reshape(1, -1)))[:, 1][0]\n",
    "            preds.append(int(y_prob >= cfg['nb_threshold']))\n",
    "            trues.append(y_test[i])\n",
    "        return self._get_rolling_f1(trues, preds, cfg['eval_step'])\n",
    "\n",
    "    def run_arf(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        print(\"\\n--- Running ARF+ADWIN (online learning) ---\")\n",
    "        cfg = self.config\n",
    "        params = cfg['arf_champion_params']\n",
    "        base_model = tree.HoeffdingTreeClassifier(grace_period=params['grace_period'], delta=params['delta'], split_criterion='hellinger')\n",
    "        forest = ensemble.BaggingClassifier(model=base_model, n_models=params['n_models'], seed=cfg['seed'])\n",
    "        detector = drift.ADWIN()\n",
    "        trues, preds = [], []\n",
    "        for _, row in self.df.iterrows():\n",
    "            x, y = row[self.feat_cols].to_dict(), int(row[cfg['target_col']])\n",
    "            y_pred = forest.predict_one(x)\n",
    "            if y_pred is not None:\n",
    "                preds.append(y_pred); trues.append(y)\n",
    "                error = int(y_pred != y)\n",
    "                detector.update(error)\n",
    "                if detector.drift_detected: forest = ensemble.BaggingClassifier(model=base_model, n_models=params['n_models'], seed=cfg['seed'])\n",
    "            forest.learn_one(x, y)\n",
    "        return self._get_rolling_f1(trues, preds, cfg['eval_step'])\n",
    "\n",
    "    def run_lstm(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        print(\"\\n--- Running LSTM (train-once) ---\")\n",
    "        X, y, cfg = self.X_seq, self.y_seq, self.config\n",
    "        train_end = int(len(y) * 0.8)\n",
    "        X_train, y_train, X_test, y_test = X[:train_end], y[:train_end], X[train_end:], y[train_end:]\n",
    "        scaler = StandardScaler().fit(X_train.reshape(-1, cfg['n_features_seq']))\n",
    "        X_train_std = scaler.transform(X_train.reshape(-1, cfg['n_features_seq'])).reshape(X_train.shape)\n",
    "        X_test_std = scaler.transform(X_test.reshape(-1, cfg['n_features_seq'])).reshape(X_test.shape)\n",
    "        print(\"   Training LSTM model...\")\n",
    "        all_lstm_params = cfg['lstm_champion_params']\n",
    "        model_arch_params, learning_rate = {k: v for k, v in all_lstm_params.items() if k != 'lr'}, all_lstm_params['lr']\n",
    "        model = LSTMWithAttention(input_size=cfg['n_features_seq'], **model_arch_params).to(DEVICE)\n",
    "        train_loader = DataLoader(TensorDataset(torch.tensor(X_train_std, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)),\n",
    "                                  batch_size=cfg['batch_size'], shuffle=True)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        loss_fn = nn.BCELoss()\n",
    "        for epoch in range(cfg['epochs']):\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                loss = loss_fn(model(xb), yb); optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_probs = model(torch.tensor(X_test_std, dtype=torch.float32).to(DEVICE)).cpu().numpy().flatten()\n",
    "        preds = (test_probs >= cfg['lstm_threshold']).astype(int)\n",
    "        return self._get_rolling_f1(y_test.tolist(), preds.tolist(), cfg['eval_step'])\n",
    "\n",
    "    def plot_final_results(self, all_results: Dict[str, Any]):\n",
    "        print(\"\\n--- Generating Final Championship Comparison Plot ---\")\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        max_f1_observed = 0.0\n",
    "\n",
    "        for name, (indices, scores) in all_results.items():\n",
    "            if len(scores) > 0 and scores.max() > max_f1_observed:\n",
    "                max_f1_observed = scores.max()\n",
    "            details = self.config['plot_styles'][name]\n",
    "            ax.plot(indices, scores, label=name, marker=details['marker'], color=details['color'], linestyle=details['linestyle'], markersize=5, linewidth=2)\n",
    "\n",
    "        ax.set_title('Model Performance Over Time: A Comparison of Adaptive Strategies', fontsize=18, weight='bold', pad=20)\n",
    "        ax.set_xlabel('Number of Samples Evaluated in Stream', fontsize=14, labelpad=10)\n",
    "        ax.set_ylabel('Cumulative F1-Score', fontsize=14, labelpad=10)\n",
    "        legend = ax.legend(title='Model Philosophy', fontsize=12, title_fontsize=13)\n",
    "        plt.setp(legend.get_title(), weight='bold')\n",
    "        ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        \n",
    "        # 🔥 FIX: Dynamically set the y-axis limit with a 10% headroom\n",
    "        ax.set_ylim(0, min(1.0, max_f1_observed * 1.1) if max_f1_observed > 0 else 1.0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"CHAMPIONSHIP_FINAL_PLOT.png\", dpi=300)\n",
    "        print(\"\\n✅ Grand Finale plot saved as 'CHAMPIONSHIP_FINAL_PLOT2.png'\")\n",
    "        plt.show()\n",
    "\n",
    "    def run_all(self):\n",
    "        \"\"\"Orchestrates the entire analysis.\"\"\"\n",
    "        all_results = {}\n",
    "        all_results['Naive Bayes (Per-Instance Adaptation)'] = self.run_naive_bayes()\n",
    "        all_results['ARF+ADWIN (Online Ensemble)'] = self.run_arf()\n",
    "        all_results['LSTM (Static Knowledge)'] = self.run_lstm()\n",
    "        \n",
    "        self.plot_final_results(all_results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {\n",
    "        \"csv_path\": r'cvm_indicators_dataset_2011-2021.csv',\n",
    "        \"id_col\": \"ID\", \"quarter_col\": \"QUARTER\", \"target_col\": \"LABEL\",\n",
    "        \"meta_cols\": [\"ID\", \"QUARTER\", \"LABEL\"],\n",
    "        \"lags\": 4, \"seed\": 42,\n",
    "        \"eval_step\": 500,\n",
    "\n",
    "        \"nb_win_size\": 200, \"nb_threshold\": 0.10,\n",
    "        \"arf_champion_params\": {'n_models': 5, 'grace_period': 281, 'delta': 2.25e-06},\n",
    "        \"lstm_champion_params\": {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.40, 'lr': 0.0013},\n",
    "        \"lstm_threshold\": 0.15,\n",
    "        \n",
    "        \"epochs\": 25, \"batch_size\": 128,\n",
    "\n",
    "        \"plot_styles\": {\n",
    "            'Naive Bayes (Per-Instance Adaptation)': {'color': 'blue', 'marker': 'o', 'linestyle': '-'},\n",
    "            'ARF+ADWIN (Online Ensemble)': {'color': 'green', 'marker': 's', 'linestyle': '--'},\n",
    "            'LSTM (Static Knowledge)': {'color': 'red', 'marker': 'x', 'linestyle': ':'},\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    visualizer = Final_Drift_Visualizer(config=CONFIG)\n",
    "    visualizer.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd2bba8-0bfe-4729-80a9-7612d637f602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
