{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27d3ac3-7379-4295-ab4d-c7037a7a047b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Loading and cleaning data...\n",
      "2. Preparing sequence data...\n",
      "3. Splitting data into Tune (60%), Validation (20%), and Test (20%) sets...\n",
      "   Tune set size: 12256\n",
      "   Validation set size: 4086\n",
      "   Test set size: 4086\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "Starting BiGRU Championship Bake-Off\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "4. Starting Optuna optimization process...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae994dc600dc4bf8b6eb36278c236da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna process finished!\n",
      "ğŸ† Best F1-score on Validation Set: 0.4384\n",
      "ğŸ† Best Hyperparameters Found: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.27658312824415443, 'lr': 0.00037063679258648243}\n",
      "\n",
      "--- Training and Evaluating Champion BiGRU Model ---\n",
      "Final training on 16342 samples...\n",
      "Final training complete.\n",
      "\n",
      "  Tuning classification threshold and calculating all metrics on test set...\n",
      "\n",
      "[Optuna-Tuned BiGRU] Final Test Set Performance:\n",
      "  Best Threshold = 0.72\n",
      "  F1-Score       = 0.3074\n",
      "  AUC            = 0.7474\n",
      "  G-Mean         = 0.3150\n",
      "  Precision      = 0.2526\n",
      "  Recall         = 0.3927\n",
      "\n",
      "BiGRU Bake-Off Complete!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# BiGRU Champion Model Bake-Off with Optuna - v2 (Corrected)\n",
    "#\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Suppress Optuna's trial info messages and other warnings\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BiGRUWithAttention(nn.Module):\n",
    "    \"\"\"A Bidirectional GRU with a basic attention mechanism.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size, hidden_size, num_layers,\n",
    "            batch_first=True, dropout=dropout, bidirectional=True\n",
    "        )\n",
    "        self.attn_layer = nn.Linear(hidden_size * 2, 1) # x2 for bidirectional\n",
    "        self.output_layer = nn.Linear(hidden_size * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        attn_weights = torch.softmax(self.attn_layer(gru_out), dim=1)\n",
    "        context = torch.sum(attn_weights * gru_out, dim=1)\n",
    "        output = self.sigmoid(self.output_layer(context))\n",
    "        return output\n",
    "\n",
    "class BiGRU_Champion_Finder:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.df = self._load_data(config['csv_path'])\n",
    "        self.feat_cols = [c for c in self.df.columns if c not in config['meta_cols']]\n",
    "        self.X_all, self.y_all = self._make_windows()\n",
    "        \n",
    "        self.X_tune, self.y_tune, \\\n",
    "        self.X_val, self.y_val, \\\n",
    "        self.X_test, self.y_test = self._split_data()\n",
    "        \n",
    "        # Standardization: Fit ONLY on tune set, transform others\n",
    "        self.scaler = StandardScaler().fit(self.X_tune.reshape(-1, self.config['n_features']))\n",
    "        self.X_tune_std = self._scale_data(self.X_tune)\n",
    "        self.X_val_std = self._scale_data(self.X_val)\n",
    "        self.X_test_std = self._scale_data(self.X_test)\n",
    "\n",
    "    # ğŸ”¥ FIX: Re-inserting the missing helper methods\n",
    "    def _load_data(self, path: str | Path) -> pd.DataFrame:\n",
    "        print(\"â”€\" * 60 + \"\\n1. Loading and cleaning data...\")\n",
    "        df = pd.read_csv(path).loc[:, ~pd.read_csv(path).columns.duplicated()]\n",
    "        req = set(self.config['meta_cols'])\n",
    "        if missing := req - set(df.columns): raise KeyError(f\"Missing cols: {missing}\")\n",
    "        df[self.config['quarter_col']] = pd.to_datetime(df[self.config['quarter_col']])\n",
    "        df.sort_values([self.config['id_col'], self.config['quarter_col']], inplace=True)\n",
    "        df = df.dropna()\n",
    "        num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        return df[list(req | set(num_cols))]\n",
    "    \n",
    "    def _make_windows(self) -> (np.ndarray, np.ndarray):\n",
    "        print(\"2. Preparing sequence data...\")\n",
    "        X, y = [], []\n",
    "        cfg = self.config\n",
    "        for _, g in self.df.groupby(cfg['id_col']):\n",
    "            g = g.sort_values(cfg['quarter_col'])\n",
    "            arr, lbl = g[self.feat_cols].to_numpy(), g[cfg['target_col']].to_numpy()\n",
    "            for i in range(cfg['lags'], len(g)):\n",
    "                # Keep the windowed format for RNNs\n",
    "                X.append(arr[i - cfg['lags']:i])\n",
    "                y.append(lbl[i])\n",
    "        return np.asarray(X), np.asarray(y)\n",
    "\n",
    "    def _split_data(self):\n",
    "        print(\"3. Splitting data into Tune (60%), Validation (20%), and Test (20%) sets...\")\n",
    "        n = len(self.y_all)\n",
    "        tune_end = int(n * 0.6)\n",
    "        val_end = int(n * 0.8)\n",
    "        \n",
    "        X_tune, y_tune = self.X_all[:tune_end], self.y_all[:tune_end]\n",
    "        X_val, y_val = self.X_all[tune_end:val_end], self.y_all[tune_end:val_end]\n",
    "        X_test, y_test = self.X_all[val_end:], self.y_all[val_end:]\n",
    "        \n",
    "        print(f\"   Tune set size: {len(y_tune)}\")\n",
    "        print(f\"   Validation set size: {len(y_val)}\")\n",
    "        print(f\"   Test set size: {len(y_test)}\")\n",
    "        return X_tune, y_tune, X_val, y_val, X_test, y_test\n",
    "\n",
    "    def _scale_data(self, X):\n",
    "        \"\"\"Helper to scale 3D sequence data correctly.\"\"\"\n",
    "        return self.scaler.transform(X.reshape(-1, self.config['n_features'])).reshape(X.shape)\n",
    "\n",
    "    def _objective(self, trial: optuna.Trial) -> float:\n",
    "        \"\"\"The objective function for Optuna to maximize.\"\"\"\n",
    "        cfg = self.config\n",
    "        params = {\n",
    "            'hidden_size': trial.suggest_categorical('hidden_size', [32, 64, 128]),\n",
    "            'num_layers': trial.suggest_int('num_layers', 1, 3),\n",
    "            'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "            'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "        }\n",
    "        \n",
    "        model = BiGRUWithAttention(\n",
    "            input_size=cfg['n_features'], \n",
    "            hidden_size=params['hidden_size'],\n",
    "            num_layers=params['num_layers'],\n",
    "            dropout=params['dropout']\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(torch.tensor(self.X_tune_std, dtype=torch.float32), \n",
    "                                                torch.tensor(self.y_tune, dtype=torch.float32).unsqueeze(1)),\n",
    "                                  batch_size=cfg['batch_size'], shuffle=True)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        # ğŸ”¥ Using pos_weight for class imbalance\n",
    "        pos_weight = torch.tensor([cfg['pos_weight_value']], device=DEVICE)\n",
    "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(cfg['epochs']):\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                preds = model(xb)\n",
    "                loss = loss_fn(preds, yb)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_probs = []\n",
    "        with torch.no_grad():\n",
    "            val_tensor = torch.tensor(self.X_val_std, dtype=torch.float32).to(DEVICE)\n",
    "            # Use sigmoid here because BCEWithLogitsLoss doesn't have it internally\n",
    "            val_probs = torch.sigmoid(model(val_tensor)).cpu().numpy().flatten()\n",
    "            \n",
    "        best_f1 = 0\n",
    "        for threshold in np.arange(0.1, 0.9, 0.05):\n",
    "            preds = (val_probs > threshold).astype(int)\n",
    "            best_f1 = max(best_f1, f1_score(self.y_val, preds))\n",
    "        \n",
    "        return best_f1\n",
    "\n",
    "    def _evaluate_champion_model(self, params: Dict[str, Any]):\n",
    "        \"\"\"Trains the champion BiGRU on all history and evaluates on the test set.\"\"\"\n",
    "        print(\"\\n--- Training and Evaluating Champion BiGRU Model ---\")\n",
    "        cfg = self.config\n",
    "        \n",
    "        X_train_final = np.vstack([self.X_tune_std, self.X_val_std])\n",
    "        y_train_final = np.concatenate([self.y_tune, self.y_val])\n",
    "\n",
    "        print(f\"Final training on {len(y_train_final)} samples...\")\n",
    "        \n",
    "        final_loader = DataLoader(TensorDataset(torch.tensor(X_train_final, dtype=torch.float32), \n",
    "                                                torch.tensor(y_train_final, dtype=torch.float32).unsqueeze(1)),\n",
    "                                  batch_size=cfg['batch_size'], shuffle=True)\n",
    "\n",
    "        model = BiGRUWithAttention(\n",
    "            input_size=cfg['n_features'],\n",
    "            hidden_size=params['hidden_size'],\n",
    "            num_layers=params['num_layers'],\n",
    "            dropout=params['dropout']\n",
    "        ).to(DEVICE)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        pos_weight = torch.tensor([cfg['pos_weight_value']], device=DEVICE)\n",
    "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(cfg['epochs_final']): # Use more epochs for final training\n",
    "            for xb, yb in final_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                loss = loss_fn(model(xb), yb)\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        print(\"Final training complete.\")\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_tensor = torch.tensor(self.X_test_std, dtype=torch.float32).to(DEVICE)\n",
    "            test_probs = torch.sigmoid(model(test_tensor)).cpu().numpy().flatten()\n",
    "        \n",
    "        print(\"\\n  Tuning classification threshold and calculating all metrics on test set...\")\n",
    "        final_auc = roc_auc_score(self.y_test, test_probs)\n",
    "        best_f1, best_thresh, best_prec, best_rec = 0, 0, 0, 0\n",
    "        for threshold in np.arange(0.1, 0.9, 0.01):\n",
    "            preds = (test_probs > threshold).astype(int)\n",
    "            current_f1 = f1_score(self.y_test, preds, zero_division=0)\n",
    "            if current_f1 > best_f1:\n",
    "                best_f1, best_thresh = current_f1, threshold\n",
    "                best_prec = precision_score(self.y_test, preds, zero_division=0)\n",
    "                best_rec = recall_score(self.y_test, preds, zero_division=0)\n",
    "        final_gmean = np.sqrt(best_prec * best_rec) if best_prec > 0 and best_rec > 0 else 0\n",
    "\n",
    "        print(f\"\\n[Optuna-Tuned BiGRU] Final Test Set Performance:\")\n",
    "        print(f\"  Best Threshold = {best_thresh:.2f}\")\n",
    "        print(f\"  F1-Score       = {best_f1:.4f}\")\n",
    "        print(f\"  AUC            = {final_auc:.4f}\")\n",
    "        print(f\"  G-Mean         = {final_gmean:.4f}\")\n",
    "        print(f\"  Precision      = {best_prec:.4f}\")\n",
    "        print(f\"  Recall         = {best_rec:.4f}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Orchestrates the entire BiGRU bake-off process.\"\"\"\n",
    "        print(\"\\n\" + \"â•\" * 60)\n",
    "        print(\"Starting BiGRU Championship Bake-Off\")\n",
    "        print(\"â•\" * 60)\n",
    "        \n",
    "        print(\"4. Starting Optuna optimization process...\")\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(self._objective, n_trials=self.config['optuna_trials'], show_progress_bar=True)\n",
    "        \n",
    "        print(f\"\\nOptuna process finished!\")\n",
    "        print(f\"ğŸ† Best F1-score on Validation Set: {study.best_value:.4f}\")\n",
    "        print(f\"ğŸ† Best Hyperparameters Found: {study.best_params}\")\n",
    "        \n",
    "        self._evaluate_champion_model(study.best_params)\n",
    "        print(\"\\nBiGRU Bake-Off Complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    temp_df = pd.read_csv(r'cvm_indicators_dataset_2011-2021.csv')\n",
    "    n_features = len(temp_df.columns) - 3 \n",
    "\n",
    "    CONFIG = {\n",
    "        \"csv_path\": r'cvm_indicators_dataset_2011-2021.csv',\n",
    "        \"id_col\": \"ID\", \"quarter_col\": \"QUARTER\", \"target_col\": \"LABEL\",\n",
    "        \"meta_cols\": [\"ID\", \"QUARTER\", \"LABEL\"],\n",
    "        \"lags\": 4, \"seed\": 42,\n",
    "        \"n_features\": n_features,\n",
    "        \n",
    "        \"optuna_trials\": 30,\n",
    "\n",
    "        \"epochs\": 15,\n",
    "        \"epochs_final\": 25, # More epochs for the final champion model\n",
    "        \"batch_size\": 128,\n",
    "        \"pos_weight_value\": 35 # Calculated from (num_negative / num_positive)\n",
    "    }\n",
    "\n",
    "    champion_finder = BiGRU_Champion_Finder(config=CONFIG)\n",
    "    champion_finder.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b7980-7499-4a09-b87b-5bb51db5d863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
