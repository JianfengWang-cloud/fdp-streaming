{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4022964-dc04-42c0-9cf5-6436a8cca734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Loading and cleaning data...\n",
      "2. Preparing sequence data...\n",
      "3. Splitting data into Tune (60%), Validation (20%), and Test (20%) sets...\n",
      "   Tune set size: 12256\n",
      "   Validation set size: 4086\n",
      "   Test set size: 4086\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "Starting ANN (MLP) Championship Bake-Off\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "4. Starting Optuna optimization process...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573b57b41cde422abd854ba44a25fe2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna process finished!\n",
      "ğŸ† Best F1-score on Validation Set: 0.4154\n",
      "ğŸ† Best Hyperparameters Found: {'hidden_dim': 64, 'dropout': 0.23424063613624518, 'lr': 0.00016735996335391325}\n",
      "\n",
      "--- Training and Evaluating Champion ANN Model ---\n",
      "Final training on 16342 samples...\n",
      "Final training complete.\n",
      "\n",
      "  Tuning classification threshold and calculating all metrics on test set...\n",
      "\n",
      "[Optuna-Tuned ANN] Final Test Set Performance:\n",
      "  Best Threshold = 0.70\n",
      "  F1-Score       = 0.3510\n",
      "  AUC            = 0.7446\n",
      "  G-Mean         = 0.3570\n",
      "  Precision      = 0.2969\n",
      "  Recall         = 0.4291\n",
      "\n",
      "ANN Bake-Off Complete!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# ANN/MLP Champion Model Bake-Off with Optuna\n",
    "#\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Suppress Optuna's trial info messages and other warnings\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"A simple Multi-Layer Perceptron for binary classification.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1) # Output raw logits\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class ANN_Champion_Finder:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.df = self._load_data(config['csv_path'])\n",
    "        self.feat_cols = [c for c in self.df.columns if c not in config['meta_cols']]\n",
    "        \n",
    "        # ğŸ”¥ For ANN, we use flattened features\n",
    "        self.X_all, self.y_all = self._make_windows(flatten=True) \n",
    "        self.config['n_features'] = self.X_all.shape[1] # Store number of features\n",
    "        \n",
    "        self.X_tune, self.y_tune, \\\n",
    "        self.X_val, self.y_val, \\\n",
    "        self.X_test, self.y_test = self._split_data()\n",
    "        \n",
    "        # ğŸ”¥ Standardization: Fit ONLY on tune set, transform others\n",
    "        self.scaler = StandardScaler().fit(self.X_tune)\n",
    "        self.X_tune_std = self.scaler.transform(self.X_tune)\n",
    "        self.X_val_std = self.scaler.transform(self.X_val)\n",
    "        self.X_test_std = self.scaler.transform(self.X_test)\n",
    "\n",
    "    def _load_data(self, path: str | Path) -> pd.DataFrame:\n",
    "        print(\"â”€\" * 60 + \"\\n1. Loading and cleaning data...\")\n",
    "        df = pd.read_csv(path).loc[:, ~pd.read_csv(path).columns.duplicated()]\n",
    "        req = set(self.config['meta_cols'])\n",
    "        if missing := req - set(df.columns): raise KeyError(f\"Missing cols: {missing}\")\n",
    "        df[self.config['quarter_col']] = pd.to_datetime(df[self.config['quarter_col']])\n",
    "        df.sort_values([self.config['id_col'], self.config['quarter_col']], inplace=True)\n",
    "        df = df.dropna()\n",
    "        num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        return df[list(req | set(num_cols))]\n",
    "    \n",
    "    def _make_windows(self, flatten: bool) -> (np.ndarray, np.ndarray):\n",
    "        print(\"2. Preparing sequence data...\")\n",
    "        X, y = [], []\n",
    "        cfg = self.config\n",
    "        for _, g in self.df.groupby(cfg['id_col']):\n",
    "            g = g.sort_values(cfg['quarter_col'])\n",
    "            arr, lbl = g[self.feat_cols].to_numpy(), g[cfg['target_col']].to_numpy()\n",
    "            for i in range(cfg['lags'], len(g)):\n",
    "                win = arr[i - cfg['lags']:i]\n",
    "                X.append(win.ravel() if flatten else win)\n",
    "                y.append(lbl[i])\n",
    "        return np.asarray(X), np.asarray(y)\n",
    "\n",
    "    def _split_data(self):\n",
    "        print(\"3. Splitting data into Tune (60%), Validation (20%), and Test (20%) sets...\")\n",
    "        n = len(self.y_all)\n",
    "        tune_end = int(n * 0.6)\n",
    "        val_end = int(n * 0.8)\n",
    "        \n",
    "        X_tune, y_tune = self.X_all[:tune_end], self.y_all[:tune_end]\n",
    "        X_val, y_val = self.X_all[tune_end:val_end], self.y_all[tune_end:val_end]\n",
    "        X_test, y_test = self.X_all[val_end:], self.y_all[val_end:]\n",
    "        \n",
    "        print(f\"   Tune set size: {len(y_tune)}\")\n",
    "        print(f\"   Validation set size: {len(y_val)}\")\n",
    "        print(f\"   Test set size: {len(y_test)}\")\n",
    "        return X_tune, y_tune, X_val, y_val, X_test, y_test\n",
    "\n",
    "    def _objective(self, trial: optuna.Trial) -> float:\n",
    "        \"\"\"The objective function for Optuna to maximize.\"\"\"\n",
    "        cfg = self.config\n",
    "        params = {\n",
    "            'hidden_dim': trial.suggest_categorical('hidden_dim', [64, 128, 256]),\n",
    "            'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "            'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "        }\n",
    "        \n",
    "        model = SimpleMLP(\n",
    "            input_dim=cfg['n_features'], \n",
    "            hidden_dim=params['hidden_dim'],\n",
    "            dropout=params['dropout']\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(torch.tensor(self.X_tune_std, dtype=torch.float32), \n",
    "                                                torch.tensor(self.y_tune, dtype=torch.float32).unsqueeze(1)),\n",
    "                                  batch_size=cfg['batch_size'], shuffle=True)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        pos_weight = torch.tensor([cfg['pos_weight_value']], device=DEVICE)\n",
    "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(cfg['epochs']):\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                preds = model(xb)\n",
    "                loss = loss_fn(preds, yb)\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_tensor = torch.tensor(self.X_val_std, dtype=torch.float32).to(DEVICE)\n",
    "            val_probs = torch.sigmoid(model(val_tensor)).cpu().numpy().flatten()\n",
    "            \n",
    "        best_f1 = 0\n",
    "        for threshold in np.arange(0.1, 0.9, 0.05):\n",
    "            preds = (val_probs > threshold).astype(int)\n",
    "            best_f1 = max(best_f1, f1_score(self.y_val, preds))\n",
    "        \n",
    "        return best_f1\n",
    "\n",
    "    def _evaluate_champion_model(self, params: Dict[str, Any]):\n",
    "        \"\"\"Trains the champion ANN on all history and evaluates on the test set.\"\"\"\n",
    "        print(\"\\n--- Training and Evaluating Champion ANN Model ---\")\n",
    "        cfg = self.config\n",
    "        \n",
    "        X_train_final = np.vstack([self.X_tune_std, self.X_val_std])\n",
    "        y_train_final = np.concatenate([self.y_tune, self.y_val])\n",
    "\n",
    "        print(f\"Final training on {len(y_train_final)} samples...\")\n",
    "        \n",
    "        final_loader = DataLoader(TensorDataset(torch.tensor(X_train_final, dtype=torch.float32), \n",
    "                                                torch.tensor(y_train_final, dtype=torch.float32).unsqueeze(1)),\n",
    "                                  batch_size=cfg['batch_size'], shuffle=True)\n",
    "\n",
    "        model = SimpleMLP(\n",
    "            input_dim=cfg['n_features'],\n",
    "            hidden_dim=params['hidden_dim'],\n",
    "            dropout=params['dropout']\n",
    "        ).to(DEVICE)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        pos_weight = torch.tensor([cfg['pos_weight_value']], device=DEVICE)\n",
    "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(cfg['epochs_final']):\n",
    "            for xb, yb in final_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                loss = loss_fn(model(xb), yb)\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        print(\"Final training complete.\")\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_tensor = torch.tensor(self.X_test_std, dtype=torch.float32).to(DEVICE)\n",
    "            test_probs = torch.sigmoid(model(test_tensor)).cpu().numpy().flatten()\n",
    "        \n",
    "        print(\"\\n  Tuning classification threshold and calculating all metrics on test set...\")\n",
    "        final_auc = roc_auc_score(self.y_test, test_probs)\n",
    "        best_f1, best_thresh, best_prec, best_rec = 0, 0, 0, 0\n",
    "        for threshold in np.arange(0.1, 0.9, 0.01):\n",
    "            preds = (test_probs > threshold).astype(int)\n",
    "            current_f1 = f1_score(self.y_test, preds, zero_division=0)\n",
    "            if current_f1 > best_f1:\n",
    "                best_f1, best_thresh = current_f1, threshold\n",
    "                best_prec = precision_score(self.y_test, preds, zero_division=0)\n",
    "                best_rec = recall_score(self.y_test, preds, zero_division=0)\n",
    "        final_gmean = np.sqrt(best_prec * best_rec) if best_prec > 0 and best_rec > 0 else 0\n",
    "\n",
    "        print(f\"\\n[Optuna-Tuned ANN] Final Test Set Performance:\")\n",
    "        print(f\"  Best Threshold = {best_thresh:.2f}\")\n",
    "        print(f\"  F1-Score       = {best_f1:.4f}\")\n",
    "        print(f\"  AUC            = {final_auc:.4f}\")\n",
    "        print(f\"  G-Mean         = {final_gmean:.4f}\")\n",
    "        print(f\"  Precision      = {best_prec:.4f}\")\n",
    "        print(f\"  Recall         = {best_rec:.4f}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Orchestrates the entire ANN bake-off process.\"\"\"\n",
    "        print(\"\\n\" + \"â•\" * 60)\n",
    "        print(\"Starting ANN (MLP) Championship Bake-Off\")\n",
    "        print(\"â•\" * 60)\n",
    "        \n",
    "        print(\"4. Starting Optuna optimization process...\")\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(self._objective, n_trials=self.config['optuna_trials'], show_progress_bar=True)\n",
    "        \n",
    "        print(f\"\\nOptuna process finished!\")\n",
    "        print(f\"ğŸ† Best F1-score on Validation Set: {study.best_value:.4f}\")\n",
    "        print(f\"ğŸ† Best Hyperparameters Found: {study.best_params}\")\n",
    "        \n",
    "        self._evaluate_champion_model(study.best_params)\n",
    "        print(\"\\nANN Bake-Off Complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {\n",
    "        \"csv_path\": r'cvm_indicators_dataset_2011-2021.csv',\n",
    "        \"id_col\": \"ID\", \"quarter_col\": \"QUARTER\", \"target_col\": \"LABEL\",\n",
    "        \"meta_cols\": [\"ID\", \"QUARTER\", \"LABEL\"],\n",
    "        \"lags\": 4, \"seed\": 42,\n",
    "        \n",
    "        \"optuna_trials\": 30,\n",
    "\n",
    "        \"epochs\": 15,\n",
    "        \"epochs_final\": 25,\n",
    "        \"batch_size\": 128,\n",
    "        \"pos_weight_value\": 35 \n",
    "    }\n",
    "\n",
    "    champion_finder = ANN_Champion_Finder(config=CONFIG)\n",
    "    champion_finder.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551a486-a613-4db5-8dac-837a02f2708e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
