{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01fa2c2e-7233-4182-9008-39885be921f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏\n",
      "1. Loading and cleaning data...\n",
      "2. Preparing sequence data...\n",
      "3. Splitting data into Tune (60%) and Test (40%) sets...\n",
      "   Tune set size: 12256\n",
      "   Test set size: 8172\n",
      "Starting Naive Bayes Final Exam...\n",
      "\n",
      "笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武\n",
      "Round 1: Evaluating Standard Baseline Naive Bayes\n",
      "笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武\n",
      "\n",
      "--- Evaluating 'Standard Naive Bayes' on the Final Test Set ---\n",
      "  Skipping at step 0: window contains only one class.\n",
      "  Skipping at step 500: window contains only one class.\n",
      "  Skipping at step 1000: window contains only one class.\n",
      "  Skipping at step 3000: window contains only one class.\n",
      "  Skipping at step 4500: window contains only one class.\n",
      "  Skipping at step 5500: window contains only one class.\n",
      "  Skipping at step 6500: window contains only one class.\n",
      "  Skipping at step 7000: window contains only one class.\n",
      "  Skipping at step 7500: window contains only one class.\n",
      "  Skipping at step 8000: window contains only one class.\n",
      "\n",
      "  Tuning classification threshold and calculating all metrics...\n",
      "\n",
      "[Standard Naive Bayes] Final Test Set Performance:\n",
      "  Best Threshold = 0.10\n",
      "  F1-Score       = 0.7602\n",
      "  AUC            = 0.8377\n",
      "  G-Mean         = 0.7686\n",
      "  Precision      = 0.8914\n",
      "  Recall         = 0.6627\n",
      "\n",
      "笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武\n",
      "Round 2: Finding and Evaluating GSCV-Tuned Naive Bayes\n",
      "笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武\n",
      "4. Starting GridSearchCV to find best params...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "GridSearchCV process finished!\n",
      "沛 Best F1-score on Tune Set: 0.0981\n",
      "沛 Best Hyperparameters Found: {'clf__var_smoothing': 1e-09}\n",
      "\n",
      "--- Evaluating 'GSCV-Tuned Naive Bayes' on the Final Test Set ---\n",
      "  Skipping at step 0: window contains only one class.\n",
      "  Skipping at step 500: window contains only one class.\n",
      "  Skipping at step 1000: window contains only one class.\n",
      "  Skipping at step 3000: window contains only one class.\n",
      "  Skipping at step 4500: window contains only one class.\n",
      "  Skipping at step 5500: window contains only one class.\n",
      "  Skipping at step 6500: window contains only one class.\n",
      "  Skipping at step 7000: window contains only one class.\n",
      "  Skipping at step 7500: window contains only one class.\n",
      "  Skipping at step 8000: window contains only one class.\n",
      "\n",
      "  Tuning classification threshold and calculating all metrics...\n",
      "\n",
      "[GSCV-Tuned Naive Bayes] Final Test Set Performance:\n",
      "  Best Threshold = 0.10\n",
      "  F1-Score       = 0.7602\n",
      "  AUC            = 0.8377\n",
      "  G-Mean         = 0.7686\n",
      "  Precision      = 0.8914\n",
      "  Recall         = 0.6627\n",
      "\n",
      "笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武\n",
      "Naive Bayes Final Exam Complete!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# Naive Bayes Final Exam: Standard vs. GSCV-Tuned\n",
    "#\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "class Naive_Bayes_Final_Exam:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.df = self._load_data(config['csv_path'])\n",
    "        self.feat_cols = [c for c in self.df.columns if c not in config['meta_cols']]\n",
    "        self.X_all, self.y_all = self._make_windows()\n",
    "        \n",
    "        self.X_tune, self.y_tune, \\\n",
    "        self.X_test, self.y_test = self._split_data()\n",
    "\n",
    "    def _load_data(self, path: str | Path) -> pd.DataFrame:\n",
    "        print(\"笏\" * 60 + \"\\n1. Loading and cleaning data...\")\n",
    "        df = pd.read_csv(path).loc[:, ~pd.read_csv(path).columns.duplicated()]\n",
    "        req = set(self.config['meta_cols'])\n",
    "        if missing := req - set(df.columns): raise KeyError(f\"Missing cols: {missing}\")\n",
    "        df[self.config['quarter_col']] = pd.to_datetime(df[self.config['quarter_col']])\n",
    "        df.sort_values([self.config['id_col'], self.config['quarter_col']], inplace=True)\n",
    "        df = df.dropna()\n",
    "        num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        return df[list(req | set(num_cols))]\n",
    "\n",
    "    def _make_windows(self) -> (np.ndarray, np.ndarray):\n",
    "        print(\"2. Preparing sequence data...\")\n",
    "        X, y = [], []\n",
    "        cfg = self.config\n",
    "        for _, g in self.df.groupby(cfg['id_col']):\n",
    "            g = g.sort_values(cfg['quarter_col'])\n",
    "            arr, lbl = g[self.feat_cols].to_numpy(), g[cfg['target_col']].to_numpy()\n",
    "            for i in range(cfg['lags'], len(g)):\n",
    "                X.append(arr[i - cfg['lags']:i].ravel())\n",
    "                y.append(lbl[i])\n",
    "        return np.asarray(X), np.asarray(y)\n",
    "\n",
    "    def _split_data(self):\n",
    "        \"\"\"Splits data chronologically into Tune and Test sets.\"\"\"\n",
    "        print(\"3. Splitting data into Tune (60%) and Test (40%) sets...\")\n",
    "        n = len(self.y_all)\n",
    "        tune_end = int(n * 0.6)\n",
    "        \n",
    "        X_tune, y_tune = self.X_all[:tune_end], self.y_all[:tune_end]\n",
    "        X_test, y_test = self.X_all[tune_end:], self.y_all[tune_end:]\n",
    "        \n",
    "        print(f\"   Tune set size: {len(y_tune)}\")\n",
    "        print(f\"   Test set size: {len(y_test)}\")\n",
    "        return X_tune, y_tune, X_test, y_test\n",
    "\n",
    "    def _evaluate_on_test_set(self, params: Dict[str, Any], model_name: str):\n",
    "        \"\"\"\n",
    "        Evaluates a Naive Bayes model on the final test set \n",
    "        using a sliding window (retraining every step) and full metrics.\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- Evaluating '{model_name}' on the Final Test Set ---\")\n",
    "        \n",
    "        win_size = self.config['sliding_win_size']\n",
    "        X_history = self.X_tune\n",
    "        y_history = self.y_tune\n",
    "        \n",
    "        all_probs, all_trues = [], []\n",
    "\n",
    "        for i in range(len(self.X_test)):\n",
    "            # Define the current training window from all data seen so far\n",
    "            X_train_current = np.vstack([X_history, self.X_test[:i]])\n",
    "            y_train_current = np.concatenate([y_history, self.y_test[:i]])\n",
    "            X_train_window = X_train_current[-win_size:]\n",
    "            y_train_window = y_train_current[-win_size:]\n",
    "\n",
    "            # Skip if the window is invalid\n",
    "            if len(np.unique(y_train_window)) < 2:\n",
    "                if i % 500 == 0: print(f\"  Skipping at step {i}: window contains only one class.\")\n",
    "                continue\n",
    "            \n",
    "            # Since Naive Bayes is very fast, we retrain at every step\n",
    "            scaler = StandardScaler().fit(X_train_window)\n",
    "            X_train_std = scaler.transform(X_train_window)\n",
    "            model = GaussianNB(**params).fit(X_train_std, y_train_window)\n",
    "\n",
    "            X_test_point = self.X_test[i].reshape(1, -1)\n",
    "            X_test_point_std = scaler.transform(X_test_point)\n",
    "            \n",
    "            y_prob = model.predict_proba(X_test_point_std)[:, 1][0]\n",
    "            all_probs.append(y_prob)\n",
    "            all_trues.append(self.y_test[i])\n",
    "            \n",
    "        print(\"\\n  Tuning classification threshold and calculating all metrics...\")\n",
    "        \n",
    "        final_auc = roc_auc_score(all_trues, all_probs)\n",
    "        best_f1, best_thresh, best_prec, best_rec = 0, 0, 0, 0\n",
    "        \n",
    "        for threshold in np.arange(0.1, 0.9, 0.01):\n",
    "            preds = (np.array(all_probs) > threshold).astype(int)\n",
    "            current_f1 = f1_score(all_trues, preds, zero_division=0)\n",
    "            if current_f1 > best_f1:\n",
    "                best_f1, best_thresh = current_f1, threshold\n",
    "                best_prec = precision_score(all_trues, preds, zero_division=0)\n",
    "                best_rec = recall_score(all_trues, preds, zero_division=0)\n",
    "        \n",
    "        final_gmean = np.sqrt(best_prec * best_rec) if best_prec > 0 and best_rec > 0 else 0\n",
    "\n",
    "        print(f\"\\n[{model_name}] Final Test Set Performance:\")\n",
    "        print(f\"  Best Threshold = {best_thresh:.2f}\")\n",
    "        print(f\"  F1-Score       = {best_f1:.4f}\")\n",
    "        print(f\"  AUC            = {final_auc:.4f}\")\n",
    "        print(f\"  G-Mean         = {final_gmean:.4f}\")\n",
    "        print(f\"  Precision      = {best_prec:.4f}\")\n",
    "        print(f\"  Recall         = {best_rec:.4f}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Orchestrates the entire Naive Bayes final exam.\"\"\"\n",
    "        print(\"Starting Naive Bayes Final Exam...\")\n",
    "        \n",
    "        # --- Model 1: Standard Baseline NB ---\n",
    "        print(\"\\n\" + \"笊申" * 60)\n",
    "        print(\"Round 1: Evaluating Standard Baseline Naive Bayes\")\n",
    "        print(\"笊申" * 60)\n",
    "        standard_params = {} # GaussianNB has no major params to set for a baseline\n",
    "        self._evaluate_on_test_set(standard_params, \"Standard Naive Bayes\")\n",
    "\n",
    "        # --- Model 2: GSCV-Tuned NB ---\n",
    "        print(\"\\n\" + \"笊申" * 60)\n",
    "        print(\"Round 2: Finding and Evaluating GSCV-Tuned Naive Bayes\")\n",
    "        print(\"笊申" * 60)\n",
    "        print(\"4. Starting GridSearchCV to find best params...\")\n",
    "        \n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('clf', GaussianNB())\n",
    "        ])\n",
    "        \n",
    "        # The main parameter to tune for GaussianNB is var_smoothing\n",
    "        param_grid = {\n",
    "            'clf__var_smoothing': np.logspace(0, -9, num=10)\n",
    "        }\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.config['seed'])\n",
    "        gs = GridSearchCV(pipeline, param_grid, scoring='f1', cv=cv, n_jobs=-1, verbose=1)\n",
    "        gs.fit(self.X_tune, self.y_tune)\n",
    "        \n",
    "        print(\"\\nGridSearchCV process finished!\")\n",
    "        print(f\"沛 Best F1-score on Tune Set: {gs.best_score_:.4f}\")\n",
    "        print(f\"沛 Best Hyperparameters Found: {gs.best_params_}\")\n",
    "        \n",
    "        gscv_params = {k.replace('clf__', ''): v for k, v in gs.best_params_.items()}\n",
    "\n",
    "        self._evaluate_on_test_set(gscv_params, \"GSCV-Tuned Naive Bayes\")\n",
    "        print(\"\\n\" + \"笊申" * 60)\n",
    "        print(\"Naive Bayes Final Exam Complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {\n",
    "        \"csv_path\": r'cvm_indicators_dataset_2011-2021.csv',\n",
    "        \"id_col\": \"ID\", \"quarter_col\": \"QUARTER\", \"target_col\": \"LABEL\",\n",
    "        \"meta_cols\": [\"ID\", \"QUARTER\", \"LABEL\"],\n",
    "        \"lags\": 4, \"seed\": 42,\n",
    "        \n",
    "        \"sliding_win_size\": 200,\n",
    "        # retrain_interval is not needed here as we retrain every step\n",
    "    }\n",
    "\n",
    "    exam_runner = Naive_Bayes_Final_Exam(config=CONFIG)\n",
    "    exam_runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df50caa-764d-42f4-ac44-e99712618bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
