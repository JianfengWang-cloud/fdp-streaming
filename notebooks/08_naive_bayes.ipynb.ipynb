{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01fa2c2e-7233-4182-9008-39885be921f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────\n",
      "1. Loading and cleaning data...\n",
      "2. Preparing sequence data...\n",
      "3. Splitting data into Tune (60%) and Test (40%) sets...\n",
      "   Tune set size: 12256\n",
      "   Test set size: 8172\n",
      "Starting Naive Bayes Final Exam...\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "Round 1: Evaluating Standard Baseline Naive Bayes\n",
      "════════════════════════════════════════════════════════════\n",
      "\n",
      "--- Evaluating 'Standard Naive Bayes' on the Final Test Set ---\n",
      "  Skipping at step 0: window contains only one class.\n",
      "  Skipping at step 500: window contains only one class.\n",
      "  Skipping at step 1000: window contains only one class.\n",
      "  Skipping at step 3000: window contains only one class.\n",
      "  Skipping at step 4500: window contains only one class.\n",
      "  Skipping at step 5500: window contains only one class.\n",
      "  Skipping at step 6500: window contains only one class.\n",
      "  Skipping at step 7000: window contains only one class.\n",
      "  Skipping at step 7500: window contains only one class.\n",
      "  Skipping at step 8000: window contains only one class.\n",
      "\n",
      "  Tuning classification threshold and calculating all metrics...\n",
      "\n",
      "[Standard Naive Bayes] Final Test Set Performance:\n",
      "  Best Threshold = 0.10\n",
      "  F1-Score       = 0.7602\n",
      "  AUC            = 0.8377\n",
      "  G-Mean         = 0.7686\n",
      "  Precision      = 0.8914\n",
      "  Recall         = 0.6627\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "Round 2: Finding and Evaluating GSCV-Tuned Naive Bayes\n",
      "════════════════════════════════════════════════════════════\n",
      "4. Starting GridSearchCV to find best params...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "GridSearchCV process finished!\n",
      "🏆 Best F1-score on Tune Set: 0.0981\n",
      "🏆 Best Hyperparameters Found: {'clf__var_smoothing': 1e-09}\n",
      "\n",
      "--- Evaluating 'GSCV-Tuned Naive Bayes' on the Final Test Set ---\n",
      "  Skipping at step 0: window contains only one class.\n",
      "  Skipping at step 500: window contains only one class.\n",
      "  Skipping at step 1000: window contains only one class.\n",
      "  Skipping at step 3000: window contains only one class.\n",
      "  Skipping at step 4500: window contains only one class.\n",
      "  Skipping at step 5500: window contains only one class.\n",
      "  Skipping at step 6500: window contains only one class.\n",
      "  Skipping at step 7000: window contains only one class.\n",
      "  Skipping at step 7500: window contains only one class.\n",
      "  Skipping at step 8000: window contains only one class.\n",
      "\n",
      "  Tuning classification threshold and calculating all metrics...\n",
      "\n",
      "[GSCV-Tuned Naive Bayes] Final Test Set Performance:\n",
      "  Best Threshold = 0.10\n",
      "  F1-Score       = 0.7602\n",
      "  AUC            = 0.8377\n",
      "  G-Mean         = 0.7686\n",
      "  Precision      = 0.8914\n",
      "  Recall         = 0.6627\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "Naive Bayes Final Exam Complete!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# Naive Bayes Final Exam: Standard vs. GSCV-Tuned\n",
    "#\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "class Naive_Bayes_Final_Exam:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.df = self._load_data(config['csv_path'])\n",
    "        self.feat_cols = [c for c in self.df.columns if c not in config['meta_cols']]\n",
    "        self.X_all, self.y_all = self._make_windows()\n",
    "        \n",
    "        self.X_tune, self.y_tune, \\\n",
    "        self.X_test, self.y_test = self._split_data()\n",
    "\n",
    "    def _load_data(self, path: str | Path) -> pd.DataFrame:\n",
    "        print(\"─\" * 60 + \"\\n1. Loading and cleaning data...\")\n",
    "        df = pd.read_csv(path).loc[:, ~pd.read_csv(path).columns.duplicated()]\n",
    "        req = set(self.config['meta_cols'])\n",
    "        if missing := req - set(df.columns): raise KeyError(f\"Missing cols: {missing}\")\n",
    "        df[self.config['quarter_col']] = pd.to_datetime(df[self.config['quarter_col']])\n",
    "        df.sort_values([self.config['id_col'], self.config['quarter_col']], inplace=True)\n",
    "        df = df.dropna()\n",
    "        num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        return df[list(req | set(num_cols))]\n",
    "\n",
    "    def _make_windows(self) -> (np.ndarray, np.ndarray):\n",
    "        print(\"2. Preparing sequence data...\")\n",
    "        X, y = [], []\n",
    "        cfg = self.config\n",
    "        for _, g in self.df.groupby(cfg['id_col']):\n",
    "            g = g.sort_values(cfg['quarter_col'])\n",
    "            arr, lbl = g[self.feat_cols].to_numpy(), g[cfg['target_col']].to_numpy()\n",
    "            for i in range(cfg['lags'], len(g)):\n",
    "                X.append(arr[i - cfg['lags']:i].ravel())\n",
    "                y.append(lbl[i])\n",
    "        return np.asarray(X), np.asarray(y)\n",
    "\n",
    "    def _split_data(self):\n",
    "        \"\"\"Splits data chronologically into Tune and Test sets.\"\"\"\n",
    "        print(\"3. Splitting data into Tune (60%) and Test (40%) sets...\")\n",
    "        n = len(self.y_all)\n",
    "        tune_end = int(n * 0.6)\n",
    "        \n",
    "        X_tune, y_tune = self.X_all[:tune_end], self.y_all[:tune_end]\n",
    "        X_test, y_test = self.X_all[tune_end:], self.y_all[tune_end:]\n",
    "        \n",
    "        print(f\"   Tune set size: {len(y_tune)}\")\n",
    "        print(f\"   Test set size: {len(y_test)}\")\n",
    "        return X_tune, y_tune, X_test, y_test\n",
    "\n",
    "    def _evaluate_on_test_set(self, params: Dict[str, Any], model_name: str):\n",
    "        \"\"\"\n",
    "        Evaluates a Naive Bayes model on the final test set \n",
    "        using a sliding window (retraining every step) and full metrics.\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- Evaluating '{model_name}' on the Final Test Set ---\")\n",
    "        \n",
    "        win_size = self.config['sliding_win_size']\n",
    "        X_history = self.X_tune\n",
    "        y_history = self.y_tune\n",
    "        \n",
    "        all_probs, all_trues = [], []\n",
    "\n",
    "        for i in range(len(self.X_test)):\n",
    "            # Define the current training window from all data seen so far\n",
    "            X_train_current = np.vstack([X_history, self.X_test[:i]])\n",
    "            y_train_current = np.concatenate([y_history, self.y_test[:i]])\n",
    "            X_train_window = X_train_current[-win_size:]\n",
    "            y_train_window = y_train_current[-win_size:]\n",
    "\n",
    "            # Skip if the window is invalid\n",
    "            if len(np.unique(y_train_window)) < 2:\n",
    "                if i % 500 == 0: print(f\"  Skipping at step {i}: window contains only one class.\")\n",
    "                continue\n",
    "            \n",
    "            # Since Naive Bayes is very fast, we retrain at every step\n",
    "            scaler = StandardScaler().fit(X_train_window)\n",
    "            X_train_std = scaler.transform(X_train_window)\n",
    "            model = GaussianNB(**params).fit(X_train_std, y_train_window)\n",
    "\n",
    "            X_test_point = self.X_test[i].reshape(1, -1)\n",
    "            X_test_point_std = scaler.transform(X_test_point)\n",
    "            \n",
    "            y_prob = model.predict_proba(X_test_point_std)[:, 1][0]\n",
    "            all_probs.append(y_prob)\n",
    "            all_trues.append(self.y_test[i])\n",
    "            \n",
    "        print(\"\\n  Tuning classification threshold and calculating all metrics...\")\n",
    "        \n",
    "        final_auc = roc_auc_score(all_trues, all_probs)\n",
    "        best_f1, best_thresh, best_prec, best_rec = 0, 0, 0, 0\n",
    "        \n",
    "        for threshold in np.arange(0.1, 0.9, 0.01):\n",
    "            preds = (np.array(all_probs) > threshold).astype(int)\n",
    "            current_f1 = f1_score(all_trues, preds, zero_division=0)\n",
    "            if current_f1 > best_f1:\n",
    "                best_f1, best_thresh = current_f1, threshold\n",
    "                best_prec = precision_score(all_trues, preds, zero_division=0)\n",
    "                best_rec = recall_score(all_trues, preds, zero_division=0)\n",
    "        \n",
    "        final_gmean = np.sqrt(best_prec * best_rec) if best_prec > 0 and best_rec > 0 else 0\n",
    "\n",
    "        print(f\"\\n[{model_name}] Final Test Set Performance:\")\n",
    "        print(f\"  Best Threshold = {best_thresh:.2f}\")\n",
    "        print(f\"  F1-Score       = {best_f1:.4f}\")\n",
    "        print(f\"  AUC            = {final_auc:.4f}\")\n",
    "        print(f\"  G-Mean         = {final_gmean:.4f}\")\n",
    "        print(f\"  Precision      = {best_prec:.4f}\")\n",
    "        print(f\"  Recall         = {best_rec:.4f}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Orchestrates the entire Naive Bayes final exam.\"\"\"\n",
    "        print(\"Starting Naive Bayes Final Exam...\")\n",
    "        \n",
    "        # --- Model 1: Standard Baseline NB ---\n",
    "        print(\"\\n\" + \"═\" * 60)\n",
    "        print(\"Round 1: Evaluating Standard Baseline Naive Bayes\")\n",
    "        print(\"═\" * 60)\n",
    "        standard_params = {} # GaussianNB has no major params to set for a baseline\n",
    "        self._evaluate_on_test_set(standard_params, \"Standard Naive Bayes\")\n",
    "\n",
    "        # --- Model 2: GSCV-Tuned NB ---\n",
    "        print(\"\\n\" + \"═\" * 60)\n",
    "        print(\"Round 2: Finding and Evaluating GSCV-Tuned Naive Bayes\")\n",
    "        print(\"═\" * 60)\n",
    "        print(\"4. Starting GridSearchCV to find best params...\")\n",
    "        \n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('clf', GaussianNB())\n",
    "        ])\n",
    "        \n",
    "        # The main parameter to tune for GaussianNB is var_smoothing\n",
    "        param_grid = {\n",
    "            'clf__var_smoothing': np.logspace(0, -9, num=10)\n",
    "        }\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.config['seed'])\n",
    "        gs = GridSearchCV(pipeline, param_grid, scoring='f1', cv=cv, n_jobs=-1, verbose=1)\n",
    "        gs.fit(self.X_tune, self.y_tune)\n",
    "        \n",
    "        print(\"\\nGridSearchCV process finished!\")\n",
    "        print(f\"🏆 Best F1-score on Tune Set: {gs.best_score_:.4f}\")\n",
    "        print(f\"🏆 Best Hyperparameters Found: {gs.best_params_}\")\n",
    "        \n",
    "        gscv_params = {k.replace('clf__', ''): v for k, v in gs.best_params_.items()}\n",
    "\n",
    "        self._evaluate_on_test_set(gscv_params, \"GSCV-Tuned Naive Bayes\")\n",
    "        print(\"\\n\" + \"═\" * 60)\n",
    "        print(\"Naive Bayes Final Exam Complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {\n",
    "        \"csv_path\": r'cvm_indicators_dataset_2011-2021.csv',\n",
    "        \"id_col\": \"ID\", \"quarter_col\": \"QUARTER\", \"target_col\": \"LABEL\",\n",
    "        \"meta_cols\": [\"ID\", \"QUARTER\", \"LABEL\"],\n",
    "        \"lags\": 4, \"seed\": 42,\n",
    "        \n",
    "        \"sliding_win_size\": 200,\n",
    "        # retrain_interval is not needed here as we retrain every step\n",
    "    }\n",
    "\n",
    "    exam_runner = Naive_Bayes_Final_Exam(config=CONFIG)\n",
    "    exam_runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df50caa-764d-42f4-ac44-e99712618bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
