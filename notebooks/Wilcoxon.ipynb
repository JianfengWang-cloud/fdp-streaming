{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "900e77b0-c70f-4e3f-9c2b-2c471204dbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading and cleaning data...\n",
      "2. Preparing flattened data windows...\n",
      "✅ Data prepared. Total sequences: 20428\n",
      "\n",
      "3. Setting up the official 'Test Stream'...\n",
      "   History buffer size: 4085\n",
      "   Official Test Stream size: 16343\n",
      "\n",
      "4. Running Naive Bayes on the test stream...\n",
      "   Evaluation stream complete.\n",
      "\n",
      "5. Calculating and saving cumulative AUC scores...\n",
      "   ✅ Successfully saved 32 AUC scores to 'nb_auc_scores.csv'\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# Statistical Test Data Generation (Part 1): Naive Bayes\n",
    "#\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "class Naive_Bayes_Score_Generator:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.df = self._load_data(config['csv_path'])\n",
    "        self.feat_cols = [c for c in self.df.columns if c not in config['meta_cols']]\n",
    "        self.X_all, self.y_all = self._make_windows()\n",
    "        print(f\"✅ Data prepared. Total sequences: {len(self.y_all)}\")\n",
    "\n",
    "    def _load_data(self, path: str | Path) -> pd.DataFrame:\n",
    "        print(\"1. Loading and cleaning data...\")\n",
    "        df = pd.read_csv(path).loc[:, ~pd.read_csv(path).columns.duplicated()]\n",
    "        req = set(self.config['meta_cols'])\n",
    "        if missing := req - set(df.columns): raise KeyError(f\"Missing cols: {missing}\")\n",
    "        df[self.config['quarter_col']] = pd.to_datetime(df[self.config['quarter_col']])\n",
    "        df.sort_values([self.config['id_col'], self.config['quarter_col']], inplace=True)\n",
    "        return df.dropna()\n",
    "\n",
    "    def _make_windows(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        print(f\"2. Preparing flattened data windows...\")\n",
    "        X, y = [], []\n",
    "        cfg = self.config\n",
    "        for _, g in self.df.groupby(cfg['id_col']):\n",
    "            arr, lbl = g[self.feat_cols].to_numpy(), g[cfg['target_col']].to_numpy()\n",
    "            for i in range(cfg['lags'], len(g)):\n",
    "                X.append(arr[i - cfg['lags']:i].ravel())\n",
    "                y.append(lbl[i])\n",
    "        return np.asarray(X), np.asarray(y)\n",
    "\n",
    "    def run_and_save_scores(self):\n",
    "        \"\"\"Runs the Naive Bayes model and saves its cumulative AUC scores.\"\"\"\n",
    "        print(\"\\n3. Setting up the official 'Test Stream'...\")\n",
    "        cfg = self.config\n",
    "        # Use first 20% of data as a 'warm-up' or history buffer\n",
    "        history_end = int(len(self.y_all) * 0.2)\n",
    "        X_history, y_history = self.X_all[:history_end], self.y_all[:history_end]\n",
    "        X_test, y_test = self.X_all[history_end:], self.y_all[history_end:]\n",
    "        print(f\"   History buffer size: {len(y_history)}\")\n",
    "        print(f\"   Official Test Stream size: {len(y_test)}\")\n",
    "\n",
    "        print(\"\\n4. Running Naive Bayes on the test stream...\")\n",
    "        win_size = cfg['nb_win_size']\n",
    "        all_probs, all_trues = [], []\n",
    "        \n",
    "        for i in range(len(X_test)):\n",
    "            current_train_X = np.vstack([X_history, X_test[:i]])[-win_size:]\n",
    "            current_train_y = np.concatenate([y_history, y_test[:i]])[-win_size:]\n",
    "            if len(np.unique(current_train_y)) < 2:\n",
    "                all_probs.append(0.5)  # Neutral probability if model can't be trained\n",
    "                all_trues.append(y_test[i])\n",
    "                continue\n",
    "            \n",
    "            scaler = StandardScaler().fit(current_train_X)\n",
    "            model = GaussianNB().fit(scaler.transform(current_train_X), current_train_y)\n",
    "            y_prob = model.predict_proba(scaler.transform(X_test[i].reshape(1, -1)))[:, 1][0]\n",
    "            all_probs.append(y_prob)\n",
    "            all_trues.append(y_test[i])\n",
    "\n",
    "        print(\"   Evaluation stream complete.\")\n",
    "        \n",
    "        print(\"\\n5. Calculating and saving cumulative AUC scores...\")\n",
    "        eval_step = cfg['eval_step']\n",
    "        auc_scores = []\n",
    "        for i in range(eval_step, len(all_trues), eval_step):\n",
    "            if len(np.unique(all_trues[:i])) < 2:\n",
    "                auc_scores.append(0.5)\n",
    "                continue\n",
    "            auc = roc_auc_score(all_trues[:i], all_probs[:i])\n",
    "            auc_scores.append(auc)\n",
    "            \n",
    "        # Save the scores to a file\n",
    "        results_df = pd.DataFrame({'nb_auc_scores': auc_scores})\n",
    "        results_df.to_csv(\"nb_auc_scores.csv\", index=False)\n",
    "        print(f\"   ✅ Successfully saved {len(auc_scores)} AUC scores to 'nb_auc_scores.csv'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {\n",
    "        \"csv_path\": r'cvm_indicators_dataset_2011-2021.csv',\n",
    "        \"id_col\": \"ID\", \"quarter_col\": \"QUARTER\", \"target_col\": \"LABEL\",\n",
    "        \"meta_cols\": [\"ID\", \"QUARTER\", \"LABEL\"],\n",
    "        \"lags\": 4, \"seed\": 42,\n",
    "        \n",
    "        # Config for Naive Bayes\n",
    "        \"nb_win_size\": 200,\n",
    "        \n",
    "        # Config for evaluation\n",
    "        \"eval_step\": 500, # Get a score every 500 samples\n",
    "    }\n",
    "    \n",
    "    score_generator = Naive_Bayes_Score_Generator(config=CONFIG)\n",
    "    score_generator.run_and_save_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b45e65c-2c1f-4e93-874e-219927d3fc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading and cleaning data...\n",
      "✅ Data prepared. Total samples to stream: 23834\n",
      "\n",
      "3. Setting up the official 'Test Stream'...\n",
      "   History buffer size: 4766\n",
      "   Official Test Stream size: 19068\n",
      "\n",
      "4. Running ARF+ADWIN on the test stream...\n",
      "   Warming up model on history buffer...\n",
      "   Evaluation stream complete.\n",
      "\n",
      "5. Calculating and saving cumulative AUC scores...\n",
      "   ✅ Successfully saved 38 AUC scores to 'arf_auc_scores.csv'\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# Statistical Test Data Generation (Part 2): ARF+ADWIN\n",
    "#\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from river import ensemble, tree, drift\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "class ARF_Score_Generator:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.df = self._load_data(config['csv_path'])\n",
    "        self.feat_cols = [c for c in self.df.columns if c not in config['meta_cols']]\n",
    "        print(f\"✅ Data prepared. Total samples to stream: {len(self.df)}\")\n",
    "\n",
    "    def _load_data(self, path: str | Path) -> pd.DataFrame:\n",
    "        print(\"1. Loading and cleaning data...\")\n",
    "        df = pd.read_csv(path).loc[:, ~pd.read_csv(path).columns.duplicated()]\n",
    "        req = set(self.config['meta_cols'])\n",
    "        if missing := req - set(df.columns): raise KeyError(f\"Missing cols: {missing}\")\n",
    "        df[self.config['quarter_col']] = pd.to_datetime(df[self.config['quarter_col']])\n",
    "        df.sort_values([self.config['id_col'], self.config['quarter_col']], inplace=True)\n",
    "        return df.dropna()\n",
    "\n",
    "    def _get_chunk_auc_scores(self, y_trues: List[int], y_probs: List[float], step: int) -> List[float]:\n",
    "        \"\"\"Calculates cumulative AUC scores at each interval.\"\"\"\n",
    "        auc_scores = []\n",
    "        for i in range(step, len(y_trues), step):\n",
    "            if len(np.unique(y_trues[:i])) < 2:\n",
    "                auc_scores.append(0.5)\n",
    "                continue\n",
    "            auc = roc_auc_score(y_trues[:i], y_probs[:i])\n",
    "            auc_scores.append(auc)\n",
    "        return auc_scores\n",
    "\n",
    "    def run_and_save_scores(self):\n",
    "        \"\"\"Runs the ARF+ADWIN model and saves its cumulative AUC scores.\"\"\"\n",
    "        print(\"\\n3. Setting up the official 'Test Stream'...\")\n",
    "        cfg = self.config\n",
    "        \n",
    "        # Use first 20% of data as a 'warm-up' or history buffer\n",
    "        history_end = int(len(self.df) * 0.2)\n",
    "        df_history = self.df.iloc[:history_end]\n",
    "        df_test = self.df.iloc[history_end:]\n",
    "        print(f\"   History buffer size: {len(df_history)}\")\n",
    "        print(f\"   Official Test Stream size: {len(df_test)}\")\n",
    "\n",
    "        print(\"\\n4. Running ARF+ADWIN on the test stream...\")\n",
    "        \n",
    "        params = cfg['arf_champion_params']\n",
    "        base_model = tree.HoeffdingTreeClassifier(grace_period=params['grace_period'], delta=params['delta'])\n",
    "        forest = ensemble.BaggingClassifier(model=base_model, n_models=params['n_models'], seed=cfg['seed'])\n",
    "        \n",
    "        # Pre-train the model on the history buffer for a fair start\n",
    "        print(\"   Warming up model on history buffer...\")\n",
    "        for _, row in df_history.iterrows():\n",
    "            forest.learn_one(row[self.feat_cols].to_dict(), int(row[cfg['target_col']]))\n",
    "            \n",
    "        all_probs, all_trues = [], []\n",
    "        # Evaluate on the official test stream\n",
    "        for _, row in df_test.iterrows():\n",
    "            x_dict = row[self.feat_cols].to_dict()\n",
    "            y_true = int(row[cfg['target_col']])\n",
    "            \n",
    "            # Prequential: Test then Train\n",
    "            y_prob = forest.predict_proba_one(x_dict).get(1, 0.5)\n",
    "            all_probs.append(y_prob)\n",
    "            all_trues.append(y_true)\n",
    "            \n",
    "            forest.learn_one(x_dict, y_true)\n",
    "\n",
    "        print(\"   Evaluation stream complete.\")\n",
    "        \n",
    "        print(\"\\n5. Calculating and saving cumulative AUC scores...\")\n",
    "        eval_step = cfg['eval_step']\n",
    "        auc_scores = self._get_chunk_auc_scores(all_trues, all_probs, eval_step)\n",
    "            \n",
    "        # Save the scores to a file\n",
    "        results_df = pd.DataFrame({'arf_auc_scores': auc_scores})\n",
    "        results_df.to_csv(\"arf_auc_scores.csv\", index=False)\n",
    "        print(f\"   ✅ Successfully saved {len(auc_scores)} AUC scores to 'arf_auc_scores.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {\n",
    "        \"csv_path\": r'cvm_indicators_dataset_2011-2021.csv',\n",
    "        \"id_col\": \"ID\", \"quarter_col\": \"QUARTER\", \"target_col\": \"LABEL\",\n",
    "        \"meta_cols\": [\"ID\", \"QUARTER\", \"LABEL\"],\n",
    "        \"seed\": 42,\n",
    "        \n",
    "        # Using the champion hyperparameters we found previously\n",
    "        \"arf_champion_params\": {'n_models': 5, 'grace_period': 281, 'delta': 2.25e-06},\n",
    "        \n",
    "        # Config for evaluation (MUST be the same as the Naive Bayes script)\n",
    "        \"eval_step\": 500, \n",
    "    }\n",
    "    \n",
    "    score_generator = ARF_Score_Generator(config=CONFIG)\n",
    "    score_generator.run_and_save_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed61d28d-4db1-4b3b-889e-35a3498409cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading and cleaning data...\n",
      "2. Preparing unified flattened data windows...\n",
      "✅ Unified data stream prepared. Total sequences: 20428\n",
      "\n",
      "3. Setting up the unified 'Test Stream' from sequence data...\n",
      "   History buffer size: 4085\n",
      "   Official Test Stream size: 16343\n",
      "\n",
      "--- Running Naive Bayes on the unified test stream ---\n",
      "\n",
      "--- Running ARF+ADWIN on the unified test stream ---\n",
      "   Warming up ARF model on history buffer...\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "4. Performing Wilcoxon Signed-Rank Test on Cumulative AUC Scores\n",
      "════════════════════════════════════════════════════════════\n",
      "   Paired Scores for Naive Bayes: ['0.611', '0.555', '0.536', '0.558', '0.546'] ...\n",
      "   Paired Scores for ARF+ADWIN:   ['0.295', '0.313', '0.442', '0.486', '0.545'] ...\n",
      "\n",
      "   Number of paired observations: 32\n",
      "   Wilcoxon T-statistic: 74.0000\n",
      "   P-value: 0.999919\n",
      "\n",
      "   Significance level (alpha): 0.05\n",
      "ℹ️ **Conclusion**: The result is not statistically significant.\n",
      "   We cannot reject the null hypothesis that the models perform equally.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# Final Statistical Test v2: Wilcoxon Test with Perfectly Aligned Data Streams\n",
    "#\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from river import ensemble, tree\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "class Statistical_Test_Runner:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        # We only need to load the dataframe once to prepare the windows\n",
    "        self.df = self._load_data(config['csv_path'])\n",
    "        self.feat_cols = [c for c in self.df.columns if c not in config['meta_cols']]\n",
    "        self.X_flat, self.y_flat = self._make_windows()\n",
    "        print(f\"✅ Unified data stream prepared. Total sequences: {len(self.y_flat)}\")\n",
    "\n",
    "    def _load_data(self, path: str | Path) -> pd.DataFrame:\n",
    "        print(\"1. Loading and cleaning data...\")\n",
    "        df = pd.read_csv(path).loc[:, ~pd.read_csv(path).columns.duplicated()]\n",
    "        req = set(self.config['meta_cols'])\n",
    "        if missing := req - set(df.columns): raise KeyError(f\"Missing cols: {missing}\")\n",
    "        df[self.config['quarter_col']] = pd.to_datetime(df[self.config['quarter_col']])\n",
    "        df.sort_values([self.config['id_col'], self.config['quarter_col']], inplace=True)\n",
    "        return df.dropna()\n",
    "\n",
    "    def _make_windows(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        print(f\"2. Preparing unified flattened data windows...\")\n",
    "        X, y = [], []\n",
    "        cfg = self.config\n",
    "        for _, g in self.df.groupby(cfg['id_col']):\n",
    "            arr, lbl = g[self.feat_cols].to_numpy(), g[cfg['target_col']].to_numpy()\n",
    "            for i in range(cfg['lags'], len(g)):\n",
    "                X.append(arr[i - cfg['lags']:i].ravel())\n",
    "                y.append(lbl[i])\n",
    "        return np.asarray(X), np.asarray(y)\n",
    "\n",
    "    def _get_chunk_auc_scores(self, y_trues: List[int], y_probs: List[float], step: int) -> List[float]:\n",
    "        \"\"\"Calculates cumulative AUC scores at each interval.\"\"\"\n",
    "        auc_scores = []\n",
    "        for i in range(step, len(y_trues) + 1, step):\n",
    "            if len(np.unique(y_trues[:i])) < 2:\n",
    "                auc_scores.append(0.5)\n",
    "                continue\n",
    "            auc = roc_auc_score(y_trues[:i], y_probs[:i])\n",
    "            auc_scores.append(auc)\n",
    "        return auc_scores\n",
    "\n",
    "    def run_naive_bayes_stream(self, X_history, y_history, X_test, y_test) -> List[float]:\n",
    "        print(\"\\n--- Running Naive Bayes on the unified test stream ---\")\n",
    "        cfg, win_size = self.config, self.config['nb_win_size']\n",
    "        all_probs, all_trues = [], []\n",
    "        \n",
    "        for i in range(len(X_test)):\n",
    "            current_train_X = np.vstack([X_history, X_test[:i]])[-win_size:]\n",
    "            current_train_y = np.concatenate([y_history, y_test[:i]])[-win_size:]\n",
    "            if len(np.unique(current_train_y)) < 2:\n",
    "                all_probs.append(0.5); all_trues.append(y_test[i]); continue\n",
    "            \n",
    "            scaler = StandardScaler().fit(current_train_X)\n",
    "            model = GaussianNB().fit(scaler.transform(current_train_X), current_train_y)\n",
    "            y_prob = model.predict_proba(scaler.transform(X_test[i].reshape(1, -1)))[:, 1][0]\n",
    "            all_probs.append(y_prob)\n",
    "            all_trues.append(y_test[i])\n",
    "            \n",
    "        return self._get_chunk_auc_scores(all_trues, all_probs, cfg['eval_step'])\n",
    "\n",
    "    def run_arf_stream(self, X_history, y_history, X_test, y_test) -> List[float]:\n",
    "        print(\"\\n--- Running ARF+ADWIN on the unified test stream ---\")\n",
    "        cfg, params = self.config, self.config['arf_champion_params']\n",
    "        base_model = tree.HoeffdingTreeClassifier(grace_period=params['grace_period'], delta=params['delta'])\n",
    "        forest = ensemble.BaggingClassifier(model=base_model, n_models=params['n_models'], seed=cfg['seed'])\n",
    "        \n",
    "        # 🔥 FIX: Pre-train the ARF model on the same history buffer as Naive Bayes\n",
    "        print(\"   Warming up ARF model on history buffer...\")\n",
    "        for i in range(len(X_history)):\n",
    "            forest.learn_one(dict(enumerate(X_history[i])), y_history[i])\n",
    "            \n",
    "        all_probs, all_trues = [], []\n",
    "        # 🔥 FIX: Evaluate on the exact same X_test stream as Naive Bayes\n",
    "        for i in range(len(X_test)):\n",
    "            x_dict = dict(enumerate(X_test[i]))\n",
    "            y_true = y_test[i]\n",
    "            y_prob = forest.predict_proba_one(x_dict).get(1, 0.5)\n",
    "            all_probs.append(y_prob)\n",
    "            all_trues.append(y_true)\n",
    "            forest.learn_one(x_dict, y_true)\n",
    "        \n",
    "        return self._get_chunk_auc_scores(all_trues, all_probs, cfg['eval_step'])\n",
    "\n",
    "    def run_test(self):\n",
    "        \"\"\"Orchestrates the comparison and runs the Wilcoxon test.\"\"\"\n",
    "        print(\"\\n3. Setting up the unified 'Test Stream' from sequence data...\")\n",
    "        history_end = int(len(self.y_flat) * 0.2)\n",
    "        X_history, y_history = self.X_flat[:history_end], self.y_flat[:history_end]\n",
    "        X_test, y_test = self.X_flat[history_end:], self.y_flat[history_end:]\n",
    "        print(f\"   History buffer size: {len(y_history)}\")\n",
    "        print(f\"   Official Test Stream size: {len(y_test)}\")\n",
    "        \n",
    "        nb_auc_scores = self.run_naive_bayes_stream(X_history, y_history, X_test, y_test)\n",
    "        arf_auc_scores = self.run_arf_stream(X_history, y_history, X_test, y_test)\n",
    "        \n",
    "        print(\"\\n\" + \"═\" * 60)\n",
    "        print(\"4. Performing Wilcoxon Signed-Rank Test on Cumulative AUC Scores\")\n",
    "        print(\"═\" * 60)\n",
    "        \n",
    "        if len(nb_auc_scores) != len(arf_auc_scores):\n",
    "            print(f\"❌ ERROR: Score lists have different lengths. NB: {len(nb_auc_scores)}, ARF: {len(arf_auc_scores)}. Cannot perform test.\")\n",
    "            return\n",
    "\n",
    "        print(f\"   Paired Scores for Naive Bayes: {[f'{s:.3f}' for s in nb_auc_scores[:5]]} ...\")\n",
    "        print(f\"   Paired Scores for ARF+ADWIN:   {[f'{s:.3f}' for s in arf_auc_scores[:5]]} ...\")\n",
    "        \n",
    "        statistic, p_value = wilcoxon(nb_auc_scores, arf_auc_scores, alternative='greater')\n",
    "        \n",
    "        print(f\"\\n   Number of paired observations: {len(nb_auc_scores)}\")\n",
    "        print(f\"   Wilcoxon T-statistic: {statistic:.4f}\")\n",
    "        print(f\"   P-value: {p_value:.6f}\")\n",
    "        \n",
    "        alpha = 0.05\n",
    "        print(f\"\\n   Significance level (alpha): {alpha}\")\n",
    "        if p_value < alpha:\n",
    "            print(\"✅ **Conclusion**: The result is statistically significant.\")\n",
    "            print(\"   We can reject the null hypothesis. The superior performance of Naive Bayes is not due to random chance.\")\n",
    "        else:\n",
    "            print(\"ℹ️ **Conclusion**: The result is not statistically significant.\")\n",
    "            print(\"   We cannot reject the null hypothesis that the models perform equally.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {\n",
    "        \"csv_path\": r'cvm_indicators_dataset_2011-2021.csv',\n",
    "        \"id_col\": \"ID\", \"quarter_col\": \"QUARTER\", \"target_col\": \"LABEL\",\n",
    "        \"meta_cols\": [\"ID\", \"QUARTER\", \"LABEL\"],\n",
    "        \"lags\": 4, \"seed\": 42,\n",
    "        \"nb_win_size\": 500,\n",
    "        \"arf_champion_params\": {'n_models': 5, 'grace_period': 281, 'delta': 2.25e-06},\n",
    "        \"eval_step\": 250,\n",
    "    }\n",
    "    \n",
    "    tester = Statistical_Test_Runner(config=CONFIG)\n",
    "    tester.run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd8b75a-a60b-437b-aca8-2e9437fc8380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading and cleaning data...\n",
      "2. Preparing flattened data windows...\n",
      "2. Preparing sequential data windows...\n",
      "✅ Data prepared. Total sequences: 20428\n",
      "✅ Using device: cuda\n",
      "\n",
      "3. Setting up the official 'Test Stream' (final 20% of data)...\n",
      "   History buffer size: 16342\n",
      "   Official Test Stream size: 4086\n",
      "\n",
      "--- Running Naive Bayes on the test stream ---\n",
      "\n",
      "--- Running LSTM on the test stream ---\n",
      "   Training LSTM model on full history...\n",
      "   Evaluating static LSTM on test stream...\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "4. Performing Wilcoxon Signed-Rank Test on Cumulative AUC Scores\n",
      "════════════════════════════════════════════════════════════\n",
      "   Paired Scores for Naive Bayes: ['0.500', '0.500', '0.387', '0.738', '0.736'] ...\n",
      "   Paired Scores for LSTM:        ['0.500', '0.500', '0.795', '0.571', '0.594'] ...\n",
      "\n",
      "   Number of paired observations: 20\n",
      "   Wilcoxon T-statistic: 153.0000\n",
      "   P-value: 0.001643\n",
      "\n",
      "   Significance level (alpha): 0.05\n",
      "✅ **Conclusion**: The result is statistically significant.\n",
      "   We can reject the null hypothesis. The superior performance of Naive Bayes is not due to random chance.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# Final Statistical Test: Naive Bayes (Adaptability) vs. LSTM (Deep Knowledge)\n",
    "#\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import wilcoxon\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LSTMWithAttention(nn.Module):\n",
    "    \"\"\"An LSTM with a basic attention mechanism.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.attn_layer = nn.Linear(hidden_size, 1)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn_weights = torch.softmax(self.attn_layer(lstm_out), dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        return self.sigmoid(self.output_layer(context))\n",
    "\n",
    "class Final_Showdown:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.df = self._load_data(config['csv_path'])\n",
    "        self.feat_cols = [c for c in self.df.columns if c not in config['meta_cols']]\n",
    "        \n",
    "        self.X_flat, self.y_flat = self._make_windows(flatten=True)\n",
    "        self.X_seq, self.y_seq = self._make_windows(flatten=False)\n",
    "        self.config['n_features_seq'] = self.X_seq.shape[2]\n",
    "        \n",
    "        print(f\"✅ Data prepared. Total sequences: {len(self.y_flat)}\")\n",
    "        print(f\"✅ Using device: {DEVICE}\")\n",
    "\n",
    "    def _load_data(self, path: str | Path) -> pd.DataFrame:\n",
    "        print(\"1. Loading and cleaning data...\")\n",
    "        df = pd.read_csv(path).loc[:, ~pd.read_csv(path).columns.duplicated()]\n",
    "        req = set(self.config['meta_cols'])\n",
    "        if missing := req - set(df.columns): raise KeyError(f\"Missing cols: {missing}\")\n",
    "        df[self.config['quarter_col']] = pd.to_datetime(df[self.config['quarter_col']])\n",
    "        df.sort_values([self.config['id_col'], self.config['quarter_col']], inplace=True)\n",
    "        return df.dropna()\n",
    "\n",
    "    def _make_windows(self, flatten: bool) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        print(f\"2. Preparing {'flattened' if flatten else 'sequential'} data windows...\")\n",
    "        X, y = [], []\n",
    "        cfg = self.config\n",
    "        for _, g in self.df.groupby(cfg['id_col']):\n",
    "            arr, lbl = g[self.feat_cols].to_numpy(), g[cfg['target_col']].to_numpy()\n",
    "            for i in range(cfg['lags'], len(g)):\n",
    "                win = arr[i - cfg['lags']:i]\n",
    "                X.append(win.ravel() if flatten else win)\n",
    "                y.append(lbl[i])\n",
    "        return np.asarray(X), np.asarray(y)\n",
    "    \n",
    "    def _get_chunk_auc_scores(self, y_trues: List[int], y_probs: List[float], step: int) -> List[float]:\n",
    "        auc_scores = []\n",
    "        for i in range(step, len(y_trues) + 1, step):\n",
    "            if len(np.unique(y_trues[:i])) < 2:\n",
    "                auc_scores.append(0.5)\n",
    "                continue\n",
    "            auc = roc_auc_score(y_trues[:i], y_probs[:i])\n",
    "            auc_scores.append(auc)\n",
    "        return auc_scores\n",
    "\n",
    "    def run_naive_bayes_stream(self, X_history, y_history, X_test, y_test) -> List[float]:\n",
    "        print(\"\\n--- Running Naive Bayes on the test stream ---\")\n",
    "        cfg, win_size = self.config, self.config['nb_win_size']\n",
    "        all_probs, all_trues = [], []\n",
    "        \n",
    "        for i in range(len(X_test)):\n",
    "            current_train_X = np.vstack([X_history, X_test[:i]])[-win_size:]\n",
    "            current_train_y = np.concatenate([y_history, y_test[:i]])[-win_size:]\n",
    "            if len(np.unique(current_train_y)) < 2:\n",
    "                all_probs.append(0.5); all_trues.append(y_test[i]); continue\n",
    "            \n",
    "            scaler = StandardScaler().fit(current_train_X)\n",
    "            model = GaussianNB().fit(scaler.transform(current_train_X), current_train_y)\n",
    "            y_prob = model.predict_proba(scaler.transform(X_test[i].reshape(1, -1)))[:, 1][0]\n",
    "            all_probs.append(y_prob)\n",
    "            all_trues.append(y_test[i])\n",
    "            \n",
    "        return self._get_chunk_auc_scores(all_trues, all_probs, cfg['eval_step'])\n",
    "\n",
    "    def run_lstm_stream(self, X_history, y_history, X_test, y_test) -> List[float]:\n",
    "        print(\"\\n--- Running LSTM on the test stream ---\")\n",
    "        cfg, params = self.config, self.config['lstm_champion_params']\n",
    "        \n",
    "        print(\"   Training LSTM model on full history...\")\n",
    "        scaler = StandardScaler().fit(X_history.reshape(-1, cfg['n_features_seq']))\n",
    "        X_history_std = scaler.transform(X_history.reshape(-1, cfg['n_features_seq'])).reshape(X_history.shape)\n",
    "        \n",
    "        model_arch_params = {k: v for k, v in params.items() if k != 'lr'}\n",
    "        learning_rate = params['lr']\n",
    "        model = LSTMWithAttention(input_size=cfg['n_features_seq'], **model_arch_params).to(DEVICE)\n",
    "        \n",
    "        loader = DataLoader(TensorDataset(torch.tensor(X_history_std, dtype=torch.float32), torch.tensor(y_history, dtype=torch.float32).unsqueeze(1)),\n",
    "                          batch_size=cfg['batch_size'], shuffle=True)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        loss_fn = nn.BCELoss()\n",
    "        \n",
    "        for epoch in range(cfg['epochs']):\n",
    "            for xb, yb in loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                loss = loss_fn(model(xb), yb); optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        \n",
    "        print(\"   Evaluating static LSTM on test stream...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_std = scaler.transform(X_test.reshape(-1, cfg['n_features_seq'])).reshape(X_test.shape)\n",
    "            test_probs = model(torch.tensor(X_test_std, dtype=torch.float32).to(DEVICE)).cpu().numpy().flatten()\n",
    "        \n",
    "        return self._get_chunk_auc_scores(y_test.tolist(), test_probs.tolist(), cfg['eval_step'])\n",
    "\n",
    "    def run_test(self):\n",
    "        \"\"\"Orchestrates the comparison and runs the Wilcoxon test.\"\"\"\n",
    "        print(\"\\n3. Setting up the official 'Test Stream' (final 20% of data)...\")\n",
    "        test_split = int(len(self.y_flat) * 0.8)\n",
    "        \n",
    "        # Data for Naive Bayes (flattened)\n",
    "        X_history_flat, y_history_flat = self.X_flat[:test_split], self.y_flat[:test_split]\n",
    "        X_test_flat, y_test_flat = self.X_flat[test_split:], self.y_flat[test_split:]\n",
    "        \n",
    "        # Data for LSTM (sequential)\n",
    "        X_history_seq, y_history_seq = self.X_seq[:test_split], self.y_seq[:test_split]\n",
    "        X_test_seq, y_test_seq = self.X_seq[test_split:], self.y_seq[test_split:]\n",
    "        \n",
    "        print(f\"   History buffer size: {len(y_history_flat)}\")\n",
    "        print(f\"   Official Test Stream size: {len(y_test_flat)}\")\n",
    "        \n",
    "        nb_auc_scores = self.run_naive_bayes_stream(X_history_flat, y_history_flat, X_test_flat, y_test_flat)\n",
    "        lstm_auc_scores = self.run_lstm_stream(X_history_seq, y_history_seq, X_test_seq, y_test_seq)\n",
    "        \n",
    "        print(\"\\n\" + \"═\" * 60)\n",
    "        print(\"4. Performing Wilcoxon Signed-Rank Test on Cumulative AUC Scores\")\n",
    "        print(\"═\" * 60)\n",
    "        \n",
    "        if len(nb_auc_scores) != len(lstm_auc_scores):\n",
    "            print(f\"❌ ERROR: Score lists have different lengths. NB: {len(nb_auc_scores)}, LSTM: {len(lstm_auc_scores)}. Cannot perform test.\")\n",
    "            return\n",
    "\n",
    "        print(f\"   Paired Scores for Naive Bayes: {[f'{s:.3f}' for s in nb_auc_scores[:5]]} ...\")\n",
    "        print(f\"   Paired Scores for LSTM:        {[f'{s:.3f}' for s in lstm_auc_scores[:5]]} ...\")\n",
    "        \n",
    "        statistic, p_value = wilcoxon(nb_auc_scores, lstm_auc_scores, alternative='greater')\n",
    "        \n",
    "        print(f\"\\n   Number of paired observations: {len(nb_auc_scores)}\")\n",
    "        print(f\"   Wilcoxon T-statistic: {statistic:.4f}\")\n",
    "        print(f\"   P-value: {p_value:.6f}\")\n",
    "        \n",
    "        alpha = 0.05\n",
    "        print(f\"\\n   Significance level (alpha): {alpha}\")\n",
    "        if p_value < alpha:\n",
    "            print(\"✅ **Conclusion**: The result is statistically significant.\")\n",
    "            print(\"   We can reject the null hypothesis. The superior performance of Naive Bayes is not due to random chance.\")\n",
    "        else:\n",
    "            print(\"ℹ️ **Conclusion**: The result is not statistically significant.\")\n",
    "            print(\"   We cannot reject the null hypothesis that the models perform equally.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {\n",
    "        \"csv_path\": r'cvm_indicators_dataset_2011-2021.csv',\n",
    "        \"id_col\": \"ID\", \"quarter_col\": \"QUARTER\", \"target_col\": \"LABEL\",\n",
    "        \"meta_cols\": [\"ID\", \"QUARTER\", \"LABEL\"],\n",
    "        \"lags\": 4, \"seed\": 42,\n",
    "        \n",
    "        \"nb_win_size\": 200,\n",
    "        \"lstm_champion_params\": {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.40, 'lr': 0.0013},\n",
    "        \"epochs\": 25, \"batch_size\": 128,\n",
    "        \n",
    "        \"eval_step\": 200, # Get a paired score every 200 samples\n",
    "    }\n",
    "    \n",
    "    tester = Final_Showdown(config=CONFIG)\n",
    "    tester.run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53d15ee-8919-4e59-b92b-5c633039ae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading and cleaning data...\n",
      "2. Preparing flattened data windows...\n",
      "2. Preparing sequential data windows...\n",
      "✅ Data prepared. Total sequences: 20428\n",
      "✅ Using device: cuda\n",
      "\n",
      "3. Setting up the official 'Test Stream' (final 20% of data)...\n",
      "   History buffer size: 16342\n",
      "   Official Test Stream size: 4086\n",
      "\n",
      "--- Running Naive Bayes on the test stream ---\n",
      "\n",
      "--- Running LSTM on the test stream ---\n",
      "   Training LSTM model on full history...\n",
      "   Evaluating static LSTM on test stream...\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "4. Performing Wilcoxon Signed-Rank Test on Cumulative AUC Scores\n",
      "════════════════════════════════════════════════════════════\n",
      "          W-val alternative     p-val       RBC    CLES\n",
      "Wilcoxon  153.0     greater  0.001762  0.789474  0.7125\n",
      "\n",
      "   Number of paired observations: 20\n",
      "   P-value: 0.001762\n",
      "   Effect Size (r): 0.7895\n",
      "\n",
      "   Significance level (alpha): 0.05\n",
      "✅ **Conclusion**: The result is statistically significant.\n",
      "   We can reject the null hypothesis. The superior performance of Naive Bayes is not due to random chance.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# Final Statistical Test v2: NB vs. LSTM with Effect Size\n",
    "#\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# 🔥 Key Import for statistical testing and effect size\n",
    "import pingouin as pg\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LSTMWithAttention(nn.Module):\n",
    "    \"\"\"An LSTM with a basic attention mechanism.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.attn_layer = nn.Linear(hidden_size, 1)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn_weights = torch.softmax(self.attn_layer(lstm_out), dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        return self.sigmoid(self.output_layer(context))\n",
    "\n",
    "class Final_Showdown:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.df = self._load_data(config['csv_path'])\n",
    "        self.feat_cols = [c for c in self.df.columns if c not in config['meta_cols']]\n",
    "        \n",
    "        self.X_flat, self.y_flat = self._make_windows(flatten=True)\n",
    "        self.X_seq, self.y_seq = self._make_windows(flatten=False)\n",
    "        self.config['n_features_seq'] = self.X_seq.shape[2]\n",
    "        \n",
    "        print(f\"✅ Data prepared. Total sequences: {len(self.y_flat)}\")\n",
    "        print(f\"✅ Using device: {DEVICE}\")\n",
    "\n",
    "    def _load_data(self, path: str | Path) -> pd.DataFrame:\n",
    "        print(\"1. Loading and cleaning data...\")\n",
    "        df = pd.read_csv(path).loc[:, ~pd.read_csv(path).columns.duplicated()]\n",
    "        req = set(self.config['meta_cols'])\n",
    "        if missing := req - set(df.columns): raise KeyError(f\"Missing cols: {missing}\")\n",
    "        df[self.config['quarter_col']] = pd.to_datetime(df[self.config['quarter_col']])\n",
    "        df.sort_values([self.config['id_col'], self.config['quarter_col']], inplace=True)\n",
    "        return df.dropna()\n",
    "\n",
    "    def _make_windows(self, flatten: bool) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        print(f\"2. Preparing {'flattened' if flatten else 'sequential'} data windows...\")\n",
    "        X, y = [], []\n",
    "        cfg = self.config\n",
    "        for _, g in self.df.groupby(cfg['id_col']):\n",
    "            arr, lbl = g[self.feat_cols].to_numpy(), g[cfg['target_col']].to_numpy()\n",
    "            for i in range(cfg['lags'], len(g)):\n",
    "                win = arr[i - cfg['lags']:i]\n",
    "                X.append(win.ravel() if flatten else win)\n",
    "                y.append(lbl[i])\n",
    "        return np.asarray(X), np.asarray(y)\n",
    "    \n",
    "    def _get_chunk_auc_scores(self, y_trues: List[int], y_probs: List[float], step: int) -> List[float]:\n",
    "        auc_scores = []\n",
    "        for i in range(step, len(y_trues) + 1, step):\n",
    "            if len(np.unique(y_trues[:i])) < 2:\n",
    "                auc_scores.append(0.5)\n",
    "                continue\n",
    "            auc = roc_auc_score(y_trues[:i], y_probs[:i])\n",
    "            auc_scores.append(auc)\n",
    "        return auc_scores\n",
    "\n",
    "    def run_naive_bayes_stream(self, X_history, y_history, X_test, y_test) -> List[float]:\n",
    "        print(\"\\n--- Running Naive Bayes on the test stream ---\")\n",
    "        cfg, win_size = self.config, self.config['nb_win_size']\n",
    "        all_probs, all_trues = [], []\n",
    "        \n",
    "        for i in range(len(X_test)):\n",
    "            current_train_X = np.vstack([X_history, X_test[:i]])[-win_size:]\n",
    "            current_train_y = np.concatenate([y_history, y_test[:i]])[-win_size:]\n",
    "            if len(np.unique(current_train_y)) < 2:\n",
    "                all_probs.append(0.5); all_trues.append(y_test[i]); continue\n",
    "            \n",
    "            scaler = StandardScaler().fit(current_train_X)\n",
    "            model = GaussianNB().fit(scaler.transform(current_train_X), current_train_y)\n",
    "            y_prob = model.predict_proba(scaler.transform(X_test[i].reshape(1, -1)))[:, 1][0]\n",
    "            all_probs.append(y_prob)\n",
    "            all_trues.append(y_test[i])\n",
    "            \n",
    "        return self._get_chunk_auc_scores(all_trues, all_probs, cfg['eval_step'])\n",
    "\n",
    "    def run_lstm_stream(self, X_history, y_history, X_test, y_test) -> List[float]:\n",
    "        print(\"\\n--- Running LSTM on the test stream ---\")\n",
    "        cfg, params = self.config, self.config['lstm_champion_params']\n",
    "        \n",
    "        print(\"   Training LSTM model on full history...\")\n",
    "        scaler = StandardScaler().fit(X_history.reshape(-1, cfg['n_features_seq']))\n",
    "        X_history_std = scaler.transform(X_history.reshape(-1, cfg['n_features_seq'])).reshape(X_history.shape)\n",
    "        \n",
    "        model_arch_params = {k: v for k, v in params.items() if k != 'lr'}\n",
    "        learning_rate = params['lr']\n",
    "        model = LSTMWithAttention(input_size=cfg['n_features_seq'], **model_arch_params).to(DEVICE)\n",
    "        \n",
    "        loader = DataLoader(TensorDataset(torch.tensor(X_history_std, dtype=torch.float32), torch.tensor(y_history, dtype=torch.float32).unsqueeze(1)),\n",
    "                          batch_size=cfg['batch_size'], shuffle=True)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        loss_fn = nn.BCELoss()\n",
    "        \n",
    "        for epoch in range(cfg['epochs']):\n",
    "            for xb, yb in loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                loss = loss_fn(model(xb), yb); optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        \n",
    "        print(\"   Evaluating static LSTM on test stream...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_std = scaler.transform(X_test.reshape(-1, cfg['n_features_seq'])).reshape(X_test.shape)\n",
    "            test_probs = model(torch.tensor(X_test_std, dtype=torch.float32).to(DEVICE)).cpu().numpy().flatten()\n",
    "        \n",
    "        return self._get_chunk_auc_scores(y_test.tolist(), test_probs.tolist(), cfg['eval_step'])\n",
    "\n",
    "    def run_test(self):\n",
    "        \"\"\"Orchestrates the comparison and runs the Wilcoxon test.\"\"\"\n",
    "        print(\"\\n3. Setting up the official 'Test Stream' (final 20% of data)...\")\n",
    "        test_split = int(len(self.y_flat) * 0.8)\n",
    "        \n",
    "        X_history_flat, y_history_flat = self.X_flat[:test_split], self.y_flat[:test_split]\n",
    "        X_test_flat, y_test_flat = self.X_flat[test_split:], self.y_flat[test_split:]\n",
    "        \n",
    "        X_history_seq, y_history_seq = self.X_seq[:test_split], self.y_seq[:test_split]\n",
    "        X_test_seq, y_test_seq = self.X_seq[test_split:], self.y_seq[test_split:]\n",
    "        \n",
    "        print(f\"   History buffer size: {len(y_history_flat)}\")\n",
    "        print(f\"   Official Test Stream size: {len(y_test_flat)}\")\n",
    "        \n",
    "        nb_auc_scores = self.run_naive_bayes_stream(X_history_flat, y_history_flat, X_test_flat, y_test_flat)\n",
    "        lstm_auc_scores = self.run_lstm_stream(X_history_seq, y_history_seq, X_test_seq, y_test_seq)\n",
    "        \n",
    "        print(\"\\n\" + \"═\" * 60)\n",
    "        print(\"4. Performing Wilcoxon Signed-Rank Test on Cumulative AUC Scores\")\n",
    "        print(\"═\" * 60)\n",
    "        \n",
    "        if len(nb_auc_scores) != len(lstm_auc_scores):\n",
    "            print(f\"❌ ERROR: Score lists have different lengths. NB: {len(nb_auc_scores)}, LSTM: {len(lstm_auc_scores)}. Cannot perform test.\")\n",
    "            return\n",
    "\n",
    "        # 🔥 FIX: Using pingouin for a more comprehensive statistical test\n",
    "        # The pingouin library makes getting the p-value and effect size very easy.\n",
    "        stats_df = pg.wilcoxon(nb_auc_scores, lstm_auc_scores, alternative='greater')\n",
    "        \n",
    "        p_value = stats_df['p-val'].iloc[0]\n",
    "        # The rank-biserial correlation is a great effect size 'r' for Wilcoxon\n",
    "        effect_size_r = stats_df['RBC'].iloc[0] \n",
    "        \n",
    "        print(stats_df) # Print the full statistical results table\n",
    "        \n",
    "        print(f\"\\n   Number of paired observations: {len(nb_auc_scores)}\")\n",
    "        print(f\"   P-value: {p_value:.6f}\")\n",
    "        print(f\"   Effect Size (r): {effect_size_r:.4f}\")\n",
    "        \n",
    "        alpha = 0.05\n",
    "        print(f\"\\n   Significance level (alpha): {alpha}\")\n",
    "        if p_value < alpha:\n",
    "            print(\"✅ **Conclusion**: The result is statistically significant.\")\n",
    "            print(\"   We can reject the null hypothesis. The superior performance of Naive Bayes is not due to random chance.\")\n",
    "        else:\n",
    "            print(\"ℹ️ **Conclusion**: The result is not statistically significant.\")\n",
    "            print(\"   We cannot reject the null hypothesis that the models perform equally.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {\n",
    "        \"csv_path\": r'cvm_indicators_dataset_2011-2021.csv',\n",
    "        \"id_col\": \"ID\", \"quarter_col\": \"QUARTER\", \"target_col\": \"LABEL\",\n",
    "        \"meta_cols\": [\"ID\", \"QUARTER\", \"LABEL\"],\n",
    "        \"lags\": 4, \"seed\": 42,\n",
    "        \n",
    "        \"nb_win_size\": 200,\n",
    "        \"lstm_champion_params\": {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.40, 'lr': 0.0013},\n",
    "        \"epochs\": 25, \"batch_size\": 128,\n",
    "        \n",
    "        \"eval_step\": 200,\n",
    "    }\n",
    "    \n",
    "    # You might need to install pingouin: pip install pingouin\n",
    "    tester = Final_Showdown(config=CONFIG)\n",
    "    tester.run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f85ee8b-c164-42de-ae26-4a81f4e0ed0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
