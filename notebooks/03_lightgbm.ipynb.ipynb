{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc68bfb-539c-48b2-8dd5-76173a5ec133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────\n",
      "1. Loading and cleaning data...\n",
      "2. Preparing sequence data...\n",
      "3. Splitting data into Tune (60%), Validation (20%), and Test (20%) sets...\n",
      "   Tune set size: 12256\n",
      "   Validation set size: 4086\n",
      "   Test set size: 4086\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "Bake-Off Round 1: Evaluating Expert-Tuned Baseline Model\n",
      "════════════════════════════════════════════════════════════\n",
      "\n",
      "--- Evaluating 'Expert-Tuned LGBM' on the Final Test Set ---\n",
      "  Retraining at test step 0...\n",
      "  Retraining at test step 500...\n",
      "  Retraining at test step 1000...\n",
      "  Retraining at test step 1500...\n",
      "  Retraining at test step 2000...\n",
      "  Retraining at test step 2500...\n",
      "  Retraining at test step 3000...\n",
      "  Retraining at test step 3500...\n",
      "  Retraining at test step 4000...\n",
      "\n",
      "  Tuning classification threshold and calculating all metrics...\n",
      "\n",
      "[Expert-Tuned LGBM] Final Test Set Performance:\n",
      "  Best Threshold = 0.87\n",
      "  F1-Score       = 0.2265\n",
      "  AUC            = 0.5749\n",
      "  G-Mean         = 0.2432\n",
      "  Precision      = 0.1660\n",
      "  Recall         = 0.3563\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "Bake-Off Round 2: Finding and Evaluating Optuna-Tuned Model\n",
      "════════════════════════════════════════════════════════════\n",
      "4. Starting Optuna optimization process...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a4b45020746e6a7e049294add3ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna process finished!\n",
      "🏆 Best F1-score on Validation Set: 0.4265\n",
      "🏆 Best Hyperparameters Found: {'n_estimators': 400, 'learning_rate': 0.010268965803862608, 'num_leaves': 146, 'scale_pos_weight': 50}\n",
      "\n",
      "--- Evaluating 'Optuna-Tuned LGBM' on the Final Test Set ---\n",
      "  Retraining at test step 0...\n",
      "  Retraining at test step 500...\n",
      "  Retraining at test step 1000...\n",
      "  Retraining at test step 1500...\n",
      "  Retraining at test step 2000...\n",
      "  Retraining at test step 2500...\n",
      "  Retraining at test step 3000...\n",
      "  Retraining at test step 3500...\n",
      "  Retraining at test step 4000...\n",
      "\n",
      "  Tuning classification threshold and calculating all metrics...\n",
      "\n",
      "[Optuna-Tuned LGBM] Final Test Set Performance:\n",
      "  Best Threshold = 0.33\n",
      "  F1-Score       = 0.2619\n",
      "  AUC            = 0.5669\n",
      "  G-Mean         = 0.2716\n",
      "  Precision      = 0.2071\n",
      "  Recall         = 0.3563\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "Bake-Off Complete!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# LightGBM Champion Model Bake-Off v3: Full Metrics Evaluation\n",
    "#\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "# 🔥 FIX: Added roc_auc_score for a complete evaluation\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "class LightGBM_Champion_Finder:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.df = self._load_data(config['csv_path'])\n",
    "        self.feat_cols = [c for c in self.df.columns if c not in config['meta_cols']]\n",
    "        self.X_all, self.y_all = self._make_windows()\n",
    "        self.X_tune, self.y_tune, self.X_val, self.y_val, self.X_test, self.y_test = self._split_data()\n",
    "\n",
    "    def _load_data(self, path: str | Path) -> pd.DataFrame:\n",
    "        print(\"─\" * 60 + \"\\n1. Loading and cleaning data...\")\n",
    "        df = pd.read_csv(path).loc[:, ~pd.read_csv(path).columns.duplicated()]\n",
    "        req = set(self.config['meta_cols'])\n",
    "        if missing := req - set(df.columns): raise KeyError(f\"Missing cols: {missing}\")\n",
    "        df[self.config['quarter_col']] = pd.to_datetime(df[self.config['quarter_col']])\n",
    "        df.sort_values([self.config['id_col'], self.config['quarter_col']], inplace=True)\n",
    "        df = df.dropna()\n",
    "        num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        return df[list(req | set(num_cols))]\n",
    "\n",
    "    def _make_windows(self) -> (np.ndarray, np.ndarray):\n",
    "        print(\"2. Preparing sequence data...\")\n",
    "        X, y = [], []\n",
    "        cfg = self.config\n",
    "        for _, g in self.df.groupby(cfg['id_col']):\n",
    "            g = g.sort_values(cfg['quarter_col'])\n",
    "            arr, lbl = g[self.feat_cols].to_numpy(), g[cfg['target_col']].to_numpy()\n",
    "            for i in range(cfg['lags'], len(g)):\n",
    "                X.append(arr[i - cfg['lags']:i].ravel())\n",
    "                y.append(lbl[i])\n",
    "        return np.asarray(X), np.asarray(y)\n",
    "\n",
    "    def _split_data(self):\n",
    "        \"\"\"Splits data chronologically into Tune, Validation, and Test sets.\"\"\"\n",
    "        print(\"3. Splitting data into Tune (60%), Validation (20%), and Test (20%) sets...\")\n",
    "        n = len(self.y_all)\n",
    "        tune_end = int(n * 0.6)\n",
    "        val_end = int(n * 0.8)\n",
    "        \n",
    "        X_tune, y_tune = self.X_all[:tune_end], self.y_all[:tune_end]\n",
    "        X_val, y_val = self.X_all[tune_end:val_end], self.y_all[tune_end:val_end]\n",
    "        X_test, y_test = self.X_all[val_end:], self.y_all[val_end:]\n",
    "        \n",
    "        print(f\"   Tune set size: {len(y_tune)}\")\n",
    "        print(f\"   Validation set size: {len(y_val)}\")\n",
    "        print(f\"   Test set size: {len(y_test)}\")\n",
    "        return X_tune, y_tune, X_val, y_val, X_test, y_test\n",
    "\n",
    "    def _objective(self, trial: optuna.Trial) -> float:\n",
    "        \"\"\"The objective function for Optuna to maximize.\"\"\"\n",
    "        params = {\n",
    "            'objective': 'binary', 'metric': 'binary_logloss', 'verbosity': -1,\n",
    "            'random_state': self.config['seed'],\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 200, 800, step=100),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-2, 0.1, log=True),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "            'scale_pos_weight': trial.suggest_int('scale_pos_weight', 20, 50),\n",
    "        }\n",
    "\n",
    "        scaler = StandardScaler().fit(self.X_tune)\n",
    "        X_tune_std = scaler.transform(self.X_tune)\n",
    "        X_val_std = scaler.transform(self.X_val)\n",
    "        model = lgb.LGBMClassifier(**params).fit(X_tune_std, self.y_tune)\n",
    "        y_probs = model.predict_proba(X_val_std)[:, 1]\n",
    "        \n",
    "        best_f1 = 0\n",
    "        for threshold in np.arange(0.1, 0.9, 0.05):\n",
    "            preds = (y_probs > threshold).astype(int)\n",
    "            best_f1 = max(best_f1, f1_score(self.y_val, preds))\n",
    "            \n",
    "        return best_f1\n",
    "    \n",
    "    def _evaluate_on_test_set(self, params: Dict[str, Any], model_name: str):\n",
    "        \"\"\"\n",
    "        Evaluates a model on the final test set, including full metrics.\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- Evaluating '{model_name}' on the Final Test Set ---\")\n",
    "        win_size, retrain_interval = self.config['sliding_win_size'], self.config['retrain_interval']\n",
    "        X_history = np.vstack([self.X_tune, self.X_val])\n",
    "        y_history = np.concatenate([self.y_tune, self.y_val])\n",
    "        \n",
    "        all_probs, all_trues = [], []\n",
    "        model, scaler = None, None\n",
    "\n",
    "        for i in range(len(self.X_test)):\n",
    "            if model is None or i % retrain_interval == 0:\n",
    "                print(f\"  Retraining at test step {i}...\")\n",
    "                X_train_current = np.vstack([X_history, self.X_test[:i]])\n",
    "                y_train_current = np.concatenate([y_history, self.y_test[:i]])\n",
    "                X_train_window, y_train_window = X_train_current[-win_size:], y_train_current[-win_size:]\n",
    "                \n",
    "                scaler = StandardScaler().fit(X_train_window)\n",
    "                X_train_std = scaler.transform(X_train_window)\n",
    "                model = lgb.LGBMClassifier(**params).fit(X_train_std, y_train_window)\n",
    "\n",
    "            X_test_point = self.X_test[i].reshape(1, -1)\n",
    "            X_test_point_std = scaler.transform(X_test_point)\n",
    "            y_prob = model.predict_proba(X_test_point_std)[:, 1][0]\n",
    "            all_probs.append(y_prob)\n",
    "            all_trues.append(self.y_test[i])\n",
    "            \n",
    "        print(\"\\n  Tuning classification threshold and calculating all metrics...\")\n",
    "        \n",
    "        # Calculate AUC (threshold-independent)\n",
    "        final_auc = roc_auc_score(all_trues, all_probs)\n",
    "        \n",
    "        # Find best F1 and its corresponding P, R\n",
    "        best_f1, best_thresh, best_prec, best_rec = 0, 0, 0, 0\n",
    "        for threshold in np.arange(0.1, 0.9, 0.01):\n",
    "            preds = (np.array(all_probs) > threshold).astype(int)\n",
    "            current_f1 = f1_score(all_trues, preds, zero_division=0)\n",
    "            if current_f1 > best_f1:\n",
    "                best_f1, best_thresh = current_f1, threshold\n",
    "                best_prec = precision_score(all_trues, preds, zero_division=0)\n",
    "                best_rec = recall_score(all_trues, preds, zero_division=0)\n",
    "        \n",
    "        # Calculate G-mean from the best P and R\n",
    "        final_gmean = np.sqrt(best_prec * best_rec) if best_prec > 0 and best_rec > 0 else 0\n",
    "\n",
    "        print(f\"\\n[{model_name}] Final Test Set Performance:\")\n",
    "        print(f\"  Best Threshold = {best_thresh:.2f}\")\n",
    "        print(f\"  F1-Score       = {best_f1:.4f}\")\n",
    "        print(f\"  AUC            = {final_auc:.4f}\")\n",
    "        print(f\"  G-Mean         = {final_gmean:.4f}\")\n",
    "        print(f\"  Precision      = {best_prec:.4f}\")\n",
    "        print(f\"  Recall         = {best_rec:.4f}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Orchestrates the entire bake-off process.\"\"\"\n",
    "        print(\"\\n\" + \"═\" * 60)\n",
    "        print(\"Bake-Off Round 1: Evaluating Expert-Tuned Baseline Model\")\n",
    "        print(\"═\" * 60)\n",
    "        expert_params = self.config['lightgbm_expert_params']\n",
    "        self._evaluate_on_test_set(expert_params, \"Expert-Tuned LGBM\")\n",
    "\n",
    "        print(\"\\n\" + \"═\" * 60)\n",
    "        print(\"Bake-Off Round 2: Finding and Evaluating Optuna-Tuned Model\")\n",
    "        print(\"═\" * 60)\n",
    "        print(\"4. Starting Optuna optimization process...\")\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(self._objective, n_trials=self.config['optuna_trials'], show_progress_bar=True)\n",
    "        \n",
    "        print(f\"\\nOptuna process finished!\")\n",
    "        print(f\"🏆 Best F1-score on Validation Set: {study.best_value:.4f}\")\n",
    "        print(f\"🏆 Best Hyperparameters Found: {study.best_params}\")\n",
    "        \n",
    "        optuna_params = {**self.config['lightgbm_expert_params'], **study.best_params}\n",
    "        self._evaluate_on_test_set(optuna_params, \"Optuna-Tuned LGBM\")\n",
    "        print(\"\\n\" + \"═\" * 60)\n",
    "        print(\"Bake-Off Complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {\n",
    "        \"csv_path\": r'cvm_indicators_dataset_2011-2021.csv',\n",
    "        \"id_col\": \"ID\", \"quarter_col\": \"QUARTER\", \"target_col\": \"LABEL\",\n",
    "        \"meta_cols\": [\"ID\", \"QUARTER\", \"LABEL\"],\n",
    "        \"lags\": 4, \"seed\": 42,\n",
    "        \n",
    "        \"sliding_win_size\": 200,\n",
    "        \"retrain_interval\": 500,\n",
    "        \"optuna_trials\": 50,\n",
    "\n",
    "        \"lightgbm_expert_params\": {\n",
    "            \"objective\": \"binary\", \"metric\": \"auc\", \"random_state\": 42, \n",
    "            \"n_estimators\": 500, \"learning_rate\": 0.05, \"verbose\": -1,\n",
    "            \"scale_pos_weight\": 35 \n",
    "        },\n",
    "    }\n",
    "\n",
    "    champion_finder = LightGBM_Champion_Finder(config=CONFIG)\n",
    "    champion_finder.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8bed11-1cd8-412a-8470-9eb1e5b3961e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
