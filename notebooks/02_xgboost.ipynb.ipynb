{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d8b6f6-9e17-47eb-a458-e95a9405e069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────\n",
      "1. Loading and cleaning data...\n",
      "2. Preparing sequence data...\n",
      "3. Splitting data into Tune (60%), Validation (20%), and Test (20%) sets...\n",
      "   Tune set size: 12256\n",
      "   Validation set size: 4086\n",
      "   Test set size: 4086\n",
      "Starting XGBoost Championship Bake-Off...\n",
      "\n",
      "--- Evaluating 'Expert-Tuned XGBM' on the Final Test Set ---\n",
      "  Retraining at test step 0...\n",
      "  Retraining at test step 500...\n",
      "  Retraining at test step 1000...\n",
      "  Retraining at test step 1500...\n",
      "  Retraining at test step 2000...\n",
      "  Retraining at test step 2500...\n",
      "  Retraining at test step 3000...\n",
      "  Retraining at test step 3500...\n",
      "  Retraining at test step 4000...\n",
      "\n",
      "  Tuning classification threshold and calculating all metrics...\n",
      "\n",
      "[Expert-Tuned XGBM] Final Test Set Performance:\n",
      "  Best Threshold = 0.46\n",
      "  F1-Score       = 0.1843\n",
      "  AUC            = 0.6236\n",
      "  G-Mean         = 0.2151\n",
      "  Precision      = 0.1216\n",
      "  Recall         = 0.3806\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "Finding and Evaluating Optuna-Tuned XGBM\n",
      "════════════════════════════════════════════════════════════\n",
      "4. Starting Optuna optimization process...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b763b2b0d7db47c3b091c5d45a0d2820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna process finished!\n",
      "🏆 Best F1-score on Validation Set: 0.5657\n",
      "🏆 Best Hyperparameters Found: {'n_estimators': 400, 'learning_rate': 0.018726925891811734, 'max_depth': 3, 'subsample': 0.7664833578190968, 'colsample_bytree': 0.8879894104083014, 'gamma': 1.3314960759500672e-06, 'scale_pos_weight': 33}\n",
      "\n",
      "--- Evaluating 'Optuna-Tuned XGBM' on the Final Test Set ---\n",
      "  Retraining at test step 0...\n",
      "  Retraining at test step 500...\n",
      "  Retraining at test step 1000...\n",
      "  Retraining at test step 1500...\n",
      "  Retraining at test step 2000...\n",
      "  Retraining at test step 2500...\n",
      "  Retraining at test step 3000...\n",
      "  Retraining at test step 3500...\n",
      "  Retraining at test step 4000...\n",
      "\n",
      "  Tuning classification threshold and calculating all metrics...\n",
      "\n",
      "[Optuna-Tuned XGBM] Final Test Set Performance:\n",
      "  Best Threshold = 0.79\n",
      "  F1-Score       = 0.2149\n",
      "  AUC            = 0.6782\n",
      "  G-Mean         = 0.2186\n",
      "  Precision      = 0.1816\n",
      "  Recall         = 0.2632\n",
      "\n",
      "Bake-Off Complete!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# XGBoost Champion Model Bake-Off with Optuna, GPU, and Full Metrics\n",
    "#\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "# 🔥 Ensuring all necessary metric functions are imported\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Suppress Optuna's trial info messages and other warnings for cleaner output\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "class XGBoost_Champion_Finder:\n",
    "    \"\"\"\n",
    "    A dedicated class to find the best XGBoost model, accelerated by GPU.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.df = self._load_data(config['csv_path'])\n",
    "        self.feat_cols = [c for c in self.df.columns if c not in config['meta_cols']]\n",
    "        self.X_all, self.y_all = self._make_windows()\n",
    "        \n",
    "        self.X_tune, self.y_tune, \\\n",
    "        self.X_val, self.y_val, \\\n",
    "        self.X_test, self.y_test = self._split_data()\n",
    "\n",
    "    def _load_data(self, path: str | Path) -> pd.DataFrame:\n",
    "        print(\"─\" * 60 + \"\\n1. Loading and cleaning data...\")\n",
    "        df = pd.read_csv(path).loc[:, ~pd.read_csv(path).columns.duplicated()]\n",
    "        req = set(self.config['meta_cols'])\n",
    "        if missing := req - set(df.columns): raise KeyError(f\"Missing cols: {missing}\")\n",
    "        df[self.config['quarter_col']] = pd.to_datetime(df[self.config['quarter_col']])\n",
    "        df.sort_values([self.config['id_col'], self.config['quarter_col']], inplace=True)\n",
    "        df = df.dropna()\n",
    "        num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        return df[list(req | set(num_cols))]\n",
    "\n",
    "    def _make_windows(self) -> (np.ndarray, np.ndarray):\n",
    "        print(\"2. Preparing sequence data...\")\n",
    "        X, y = [], []\n",
    "        cfg = self.config\n",
    "        for _, g in self.df.groupby(cfg['id_col']):\n",
    "            g = g.sort_values(cfg['quarter_col'])\n",
    "            arr, lbl = g[self.feat_cols].to_numpy(), g[cfg['target_col']].to_numpy()\n",
    "            for i in range(cfg['lags'], len(g)):\n",
    "                X.append(arr[i - cfg['lags']:i].ravel())\n",
    "                y.append(lbl[i])\n",
    "        return np.asarray(X), np.asarray(y)\n",
    "\n",
    "    def _split_data(self):\n",
    "        \"\"\"Splits data chronologically into Tune, Validation, and Test sets.\"\"\"\n",
    "        print(\"3. Splitting data into Tune (60%), Validation (20%), and Test (20%) sets...\")\n",
    "        n = len(self.y_all)\n",
    "        tune_end = int(n * 0.6)\n",
    "        val_end = int(n * 0.8)\n",
    "        \n",
    "        X_tune, y_tune = self.X_all[:tune_end], self.y_all[:tune_end]\n",
    "        X_val, y_val = self.X_all[tune_end:val_end], self.y_all[tune_end:val_end]\n",
    "        X_test, y_test = self.X_all[val_end:], self.y_all[val_end:]\n",
    "        \n",
    "        print(f\"   Tune set size: {len(y_tune)}\")\n",
    "        print(f\"   Validation set size: {len(y_val)}\")\n",
    "        print(f\"   Test set size: {len(y_test)}\")\n",
    "        return X_tune, y_tune, X_val, y_val, X_test, y_test\n",
    "\n",
    "    def _objective(self, trial: optuna.Trial) -> float:\n",
    "        \"\"\"The objective function for Optuna to maximize, specifically for XGBoost.\"\"\"\n",
    "        params = {\n",
    "            'objective': 'binary:logistic', 'eval_metric': 'logloss', 'verbosity': 0,\n",
    "            'tree_method': 'gpu_hist',\n",
    "            'random_state': self.config['seed'],\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 200, 1000, step=100),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "            'scale_pos_weight': trial.suggest_int('scale_pos_weight', 20, 50),\n",
    "        }\n",
    "\n",
    "        model = xgb.XGBClassifier(**params).fit(self.X_tune, self.y_tune)\n",
    "        y_probs = model.predict_proba(self.X_val)[:, 1]\n",
    "        \n",
    "        best_f1 = 0\n",
    "        for threshold in np.arange(0.1, 0.9, 0.05):\n",
    "            preds = (y_probs > threshold).astype(int)\n",
    "            best_f1 = max(best_f1, f1_score(self.y_val, preds))\n",
    "            \n",
    "        return best_f1\n",
    "    \n",
    "    def _evaluate_on_test_set(self, params: Dict[str, Any], model_name: str):\n",
    "        \"\"\"Evaluates an XGBoost model on the final test set with full metrics.\"\"\"\n",
    "        print(f\"\\n--- Evaluating '{model_name}' on the Final Test Set ---\")\n",
    "        \n",
    "        win_size = self.config['sliding_win_size']\n",
    "        retrain_interval = self.config['retrain_interval']\n",
    "        X_history = np.vstack([self.X_tune, self.X_val])\n",
    "        y_history = np.concatenate([self.y_tune, self.y_val])\n",
    "        \n",
    "        all_probs, all_trues = [], []\n",
    "        model = None\n",
    "\n",
    "        for i in range(len(self.X_test)):\n",
    "            if model is None or i % retrain_interval == 0:\n",
    "                print(f\"  Retraining at test step {i}...\")\n",
    "                X_train_current = np.vstack([X_history, self.X_test[:i]])\n",
    "                y_train_current = np.concatenate([y_history, self.y_test[:i]])\n",
    "                X_train_window, y_train_window = X_train_current[-win_size:], y_train_current[-win_size:]\n",
    "                \n",
    "                model = xgb.XGBClassifier(**params).fit(X_train_window, y_train_window)\n",
    "\n",
    "            X_test_point = self.X_test[i].reshape(1, -1)\n",
    "            y_prob = model.predict_proba(X_test_point)[:, 1][0]\n",
    "            all_probs.append(y_prob)\n",
    "            all_trues.append(self.y_test[i])\n",
    "            \n",
    "        print(\"\\n  Tuning classification threshold and calculating all metrics...\")\n",
    "        \n",
    "        final_auc = roc_auc_score(all_trues, all_probs)\n",
    "        best_f1, best_thresh, best_prec, best_rec = 0, 0, 0, 0\n",
    "        \n",
    "        for threshold in np.arange(0.1, 0.9, 0.01):\n",
    "            preds = (np.array(all_probs) > threshold).astype(int)\n",
    "            current_f1 = f1_score(all_trues, preds, zero_division=0)\n",
    "            if current_f1 > best_f1:\n",
    "                best_f1, best_thresh = current_f1, threshold\n",
    "                best_prec = precision_score(all_trues, preds, zero_division=0)\n",
    "                best_rec = recall_score(all_trues, preds, zero_division=0)\n",
    "        \n",
    "        final_gmean = np.sqrt(best_prec * best_rec) if best_prec > 0 and best_rec > 0 else 0\n",
    "\n",
    "        print(f\"\\n[{model_name}] Final Test Set Performance:\")\n",
    "        print(f\"  Best Threshold = {best_thresh:.2f}\")\n",
    "        print(f\"  F1-Score       = {best_f1:.4f}\")\n",
    "        print(f\"  AUC            = {final_auc:.4f}\")\n",
    "        print(f\"  G-Mean         = {final_gmean:.4f}\")\n",
    "        print(f\"  Precision      = {best_prec:.4f}\")\n",
    "        print(f\"  Recall         = {best_rec:.4f}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Orchestrates the entire XGBoost bake-off process.\"\"\"\n",
    "        print(\"Starting XGBoost Championship Bake-Off...\")\n",
    "        \n",
    "        # --- Model 1: Expert-Tuned Baseline ---\n",
    "        expert_params = self.config['xgboost_expert_params']\n",
    "        self._evaluate_on_test_set(expert_params, \"Expert-Tuned XGBM\")\n",
    "\n",
    "        # --- Model 2: Optuna-Optimized Champion ---\n",
    "        print(\"\\n\" + \"═\" * 60)\n",
    "        print(\"Finding and Evaluating Optuna-Tuned XGBM\")\n",
    "        print(\"═\" * 60)\n",
    "        print(\"4. Starting Optuna optimization process...\")\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(self._objective, n_trials=self.config['optuna_trials'], show_progress_bar=True)\n",
    "        \n",
    "        print(f\"\\nOptuna process finished!\")\n",
    "        print(f\"🏆 Best F1-score on Validation Set: {study.best_value:.4f}\")\n",
    "        print(f\"🏆 Best Hyperparameters Found: {study.best_params}\")\n",
    "        \n",
    "        optuna_params = {**self.config['xgboost_expert_params'], **study.best_params}\n",
    "        self._evaluate_on_test_set(optuna_params, \"Optuna-Tuned XGBM\")\n",
    "        print(\"\\nBake-Off Complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {\n",
    "        \"csv_path\": r'cvm_indicators_dataset_2011-2021.csv',\n",
    "        \"id_col\": \"ID\", \"quarter_col\": \"QUARTER\", \"target_col\": \"LABEL\",\n",
    "        \"meta_cols\": [\"ID\", \"QUARTER\", \"LABEL\"],\n",
    "        \"lags\": 4, \"seed\": 42,\n",
    "        \n",
    "        \"sliding_win_size\": 200,\n",
    "        \"retrain_interval\": 500,\n",
    "\n",
    "        \"optuna_trials\": 50,\n",
    "\n",
    "        # Expert-tuned parameters for XGBoost, including GPU settings\n",
    "        \"xgboost_expert_params\": {\n",
    "            \"objective\": 'binary:logistic', \"eval_metric\": 'logloss', \"verbosity\": 0,\n",
    "            \"tree_method\": 'gpu_hist', 'predictor': 'gpu_predictor', \n",
    "            \"random_state\": 42, \"n_estimators\": 500, \"learning_rate\": 0.05,\n",
    "            \"scale_pos_weight\": 35 \n",
    "        },\n",
    "    }\n",
    "\n",
    "    champion_finder = XGBoost_Champion_Finder(config=CONFIG)\n",
    "    champion_finder.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa052bc-f481-4b84-bb06-f429faa748a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
